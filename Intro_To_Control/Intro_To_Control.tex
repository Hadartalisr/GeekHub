\documentclass[a4 paper]{article}
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,tikz,amssymb}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing} 
\usetikzlibrary{positioning}
\usepackage{float}
\usepackage{minted}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
% \usepackage[hidelinks]{hyperref}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{mathdesign}
\usepackage{float}
\usepackage{todonotes} 
\usepackage{empheq}
\usepackage{array}
\usepackage[ruled,vlined]{algorithm2e} 
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\usepackage{bookmark}

\usepackage{pgfplots} 
\pgfplotsset{compat=1.18}


\newtcolorbox{boxA}{
    fontupper = \bf,
    boxrule = 1.5pt,
    colframe = black % frame color
}


\setlength{\parindent}{0pt}
\numberwithin{equation}{section}


\newcommand\mycommfont[1]{\footnotesize\ttfamily\tblu{#1}}
\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\SetCommentSty{mycommfont}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}



\newtheoremstyle{boldStyle}%                % Name
  {}%                                     % Space above
  {}%                                     % Space below
  {\itshape}%                                     % Body font
  {}%                                     % Indent amount
  {\bfseries}%                            % Theorem head font
  {}%                                    % Punctuation after theorem head
  {\newline}                              % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%                                     % Theorem head spec (can be left empty, meaning `normal')


\theoremstyle{boldStyle}
\newtheorem{question}{Question}[section]
\newtheorem{answer}{Answer}[section]


\newtheorem*{claim*}{Claim}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\newtheorem*{example*}{Example}
\newtheorem*{examples*}{Examples}
\newtheorem*{definition*}{Definition}
\newtheorem*{question*}{Question}
\newtheorem*{answer*}{Answer}

\newtheorem{remark}{Remark}[section]


\newtheoremstyle{boldBlueStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{blueColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldBlueStyle}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{proposition}{Proposition}[section]



\newtheoremstyle{boldPurpleStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{purpleColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldPurpleStyle}
\newtheorem{theorem}{Theorem}[section]


\newtheoremstyle{boldRedStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{redColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldRedStyle}
\newtheorem{definition}{Definition}[section]


\newtheoremstyle{boldGreenStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{greenColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldGreenStyle}
\newtheorem{example}{Example}[section]


%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={Ashudeep Singh},%
pdftitle={Homework},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
% \usepackage[francais]{babel}
\usepackage{booktabs}
\input{../Latex_Utils/macros.tex}

\setlength{\parindent}{0pt}


\begin{document}
\homework{67678 - Introduction to Control with Learning}{Linear Dynamical Systems - Summary}{Spring 2024}
{Dr. Oron Sabag}{}{Hadar Tal}{}

% \maketitle
\tableofcontents


% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Mathematical Tools}


% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\subsection{Completing The Square}

\begin{lemma}[Completing the Square (scalars)]
  To complete the square for a quadratic equation of the form
  \[
  ax^2 + bx + c = 0,
  \]
  we can rewrite it as
  \[
  a(x + d)^2 + e = 0,
  \]
  where
  \[
  d = \frac{b}{2a} \quad \text{and} \quad e = c - \frac{b^2}{4a}.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    ax^2 + bx + c &= 0 &\rightarrow\quad & x^2 + \frac{b}{a}x + \frac{c}{a} = 0 &\rightarrow\quad& x^2 + 
    \frac{b}{a}x = -\frac{c}{a} \quad \rightarrow \\
    x^2 + \frac{b}{a}x + \left( \frac{b}{2a} \right)^2 &= -\frac{c}{a} + \left( \frac{b}{2a} \right)^2 
    &\rightarrow\quad& \left( x + \frac{b}{2a} \right)^2 = \frac{b^2}{4a^2} - \frac{c}{a} &\rightarrow\quad& 
    \left( x + \frac{b}{2a} \right)^2 = \frac{b^2 - 4ac}{4a^2} \quad \rightarrow \\
    a\left( x + \frac{b}{2a} \right)^2 &= a\left( \frac{b^2 - 4ac}{4a^2} \right) 
    &\rightarrow\quad& a\left( x + \frac{b}{2a} \right)^2 = \frac{b^2 - 4ac}{4a} 
    &\rightarrow\quad& a\left( x + \frac{b}{2a} \right)^2 + c - \frac{b^2}{4a} = 0
    \end{align*}
\end{proof}

\begin{lemma}[Completing the Square for Quadratic Forms]
  Given a quadratic form \( x^T A x + b^T x + c \), where \( A \) is a symmetric positive definite matrix, \( b \) is a vector, 
  and \( c \) is a scalar, the expression can be completed to a perfect square as follows:
  \begin{align*}
    x^T A x + b^T x + c &= (x + \tblu{A^{-1} b/2})^T A (x + \tblu{A^{-1} b/2}) + c \text{ } \tred{ - \frac{1}{4} b^T A^{-1} b}
  \end{align*}
\end{lemma}









% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\subsection{The Z-transform}

The Z-transform converts a discrete-time signal, which is a sequence of real or complex numbers, into a complex frequency domain representation. 

The Z-transform of a discrete-time signal \( x[k] \) is defined as:

\[
X(z) = \mathcal{Z}\{x[k]\} = \sum_{k=-\infty}^{\infty} x[k] z^{-k},
\]

where \( z \) is a complex variable. The Z-transform is particularly useful for analyzing linear time-invariant (LTI) systems.

\subsubsection{Properties of the Z-transform}
\begin{enumerate}
  \item \textbf{Linearity:}
  \[
  \mathcal{Z}\{a x[k] + b y[k]\} = a X(z) + b Y(z).
  \]

  \item \textbf{Time Shifting:}
  \[
  \mathcal{Z}\{x[k-n]\} = z^{-n} X(z).
  \]

  \item \textbf{Convolution:}
  \[
  \mathcal{Z}\{x[k] * y[k]\} = X(z) Y(z).
  \]

  \item \textbf{Initial Value Theorem:}
  \[
  x[0] = \lim_{z \to \infty} X(z).
  \]

  \item \textbf{Final Value Theorem:}
  \[
  \lim_{k \to \infty} x[k] = \lim_{z \to 1} (1 - z^{-1}) X(z),
  \]
  provided the limits exist.
\end{enumerate}



\subsubsection{Differences Between Z-transform and Laplace Transform}

\begin{enumerate}
  \item \textbf{Domain:}
  The Z-transform is used for discrete-time signals, while the Laplace transform is used for continuous-time signals.

  \item \textbf{Definition:}
  The Z-transform is defined as a summation:
  \[
  X(z) = \sum_{k=-\infty}^{\infty} x[k] z^{-k},
  \]
  while the Laplace transform is defined as an integral:
  \[
  X(s) = \int_{0}^{\infty} x(t) e^{-st} \, dt.
  \]

  \item \textbf{Complex Variable:}
  The Z-transform uses the complex variable \( z \), typically represented as \( z = e^{sT} \) where \( T \) is the sampling period. The Laplace transform uses the complex variable \( s \).

  \item \textbf{Application:}
  The Z-transform is applied to discrete-time systems and signals, making it useful for digital signal processing and discrete control systems. The Laplace transform is applied to continuous-time systems and signals, making it useful for analog signal processing and continuous control systems.
\end{enumerate}



% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{The Shortest Path Problem}

\begin{definition}[The Shortest Path Problem]
  Given a directed graph \( G = (V, E) \), each edge in the graph has a cost \( a^t_{ij} \) where 
  \( i \) is the outgoing node, \( j \) is the node to which the edge is connected, 
  and \( t \in \{0, \ldots, N+1\} \) refers to the time. We adopt the convention that no edge implies 
  an infinite cost \( a^t_{ij} = \infty \).

  The objective is to minimize the cumulative cost on a path from the source node \( S_0 = S \) to the 
  terminal node \( S_{N+1} = T \). Formally, we aim to solve the optimization
  \begin{equation}
      J^* = \min_{\{n_i \in \mathcal{S}_i\}_{i=0}^{N+1}} \sum_{t=0}^{N} a^t_{n_t, n_{t+1}}.
  \end{equation}
\end{definition}

\begin{definition}[Cost-to-Go Function]
  We define \( J_k(i) \) as the cost-to-go function corresponding to the minimal cost from time \( k \) 
  until the end when starting at node \( i \). Formally, for \( k = 0, \ldots, N \), define
  \begin{equation}
      J_k(i) = \min_{\{n_j \in \mathcal{S}_j | j=k+1,\ldots,N,n_k=i\}} \sum_{j=k}^{N} a^j_{n_t, n_{t+1}}, 
      \quad \forall i \in \mathcal{S}_k.
  \end{equation}
\end{definition}


\begin{algorithm}[H]
    \SetNoFillComment
    \SetAlgoLined
    \KwIn{Cost matrix \( a^t_{ij} \) and nodes \( \mathcal{S}_0, \mathcal{S}_1, \ldots, \mathcal{S}_{N+1} \)}
    \KwOut{Cost-to-go functions \( J_k(i) \)}
    Initialize \( J_N(i) = a^N_{iT} \) \;
    \For{$k = N-1, \ldots, 0$}{
        \For{$i \in \mathcal{S}_k$}{
            $J_k(i) = \min_{j \in \mathcal{S}_{k+1}} \left[ a^k_{ij} + J_{k+1}(j) \right]$
        }
    }
    \caption{\tpur{Dynamic Programming Solution for the Shortest Path Problem (Cost-to-Go)}} \label{algo:dp_solution_shortest_path}
\end{algorithm}


\begin{definition}[Cost-to-Arrive Function]
  We define \( J_{N-k}(j) \) as the cost-to-arrive function corresponding to the minimal cost from time \( 1 \) 
  until time \( k \) when arriving at node \( j \). Formally, for \( k = 0, \ldots, N \), define
  \begin{equation}
      J_{N-k}(j) = \min_{\{n_i \in \mathcal{S}_i | i=1,\ldots,k-1,n_k=j\}} \sum_{i=1}^{k} a^i_{n_{i-1}, n_i}, 
      \quad \forall j \in \mathcal{S}_k.
  \end{equation}
\end{definition}

\begin{algorithm}[H]
    \SetNoFillComment
    \SetAlgoLined
    \KwIn{Cost matrix \( a^t_{ij} \) and nodes \( \mathcal{S}_0, \mathcal{S}_1, \ldots, \mathcal{S}_{N+1} \)}
    \KwOut{Cost-to-arrive functions \( J_{N-k}(j) \)}
    Initialize \( J_N(j) = a^0_{sj} \), \( \forall j \in \mathcal{S}_1 \) \;
    \For{$k = 1, \ldots, N$}{
        \For{$j \in \mathcal{S}_{N-k+1}$}{
            $J_k(j) = \min_{i \in \mathcal{S}_{N-k}} \left[ a^{N-k}_{ij} + J_{k+1}(i) \right]$
        }
    }
    \caption{\tpur{Forward Algorithm for the Shortest Path Problem (Cost-to-Arrive)}} \label{algo:forward_algorithm_shortest_path}
\end{algorithm}



% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Markov Decision Processes (MDPs)}

\begin{definition}[MDP]
  An MDP is defined by the following elements:
  \begin{enumerate}
    \item The state at time \( k \) is \( x_k \) and takes values in the set \( \mathcal{S}_k \).
    \item The action at time \( k \) is \( u_k \) and takes values from \( \mathcal{U}_k \).
    \item The disturbance at time \( k \) is \( w_k \) and takes values from \( \mathcal{W}_k \).
    \item A dynamical system is given by the function
      \begin{equation}
        x_{k+1} = f_k(x_k, u_k, w_k), \quad k = 0, \ldots, N-1.
      \end{equation}
    \item The probabilistic law of the disturbance random variable \( w_k \) is characterized by \( P_{W_k}(\cdot | x_k, u_k) \) conditioned on the state \( x_k \) and the action \( u_k \).
    \item A cost function \( g_k : \mathcal{S}_k \times \mathcal{U}_k \to \mathbb{R} \).
  \end{enumerate}
\end{definition}

The cost over a horizon \( N \) is
\begin{equation}
  S_\pi(x_0) = \mathbb{E}[g_N(x_N) + \sum_{k=0}^{N-1} g_k(x_k, u_k)],
\end{equation}
where \( g_N(\cdot) \) is the terminal cost.

\begin{definition}[History-dependent policy]
  A history-dependent policy is defined by a sequence of functions:
  \begin{equation}
    \mu_k : \mathcal{S}_1 \times \cdots \times \mathcal{S}_k \times \mathcal{U}_1 \times \cdots \mathcal{U}_{k-1} \to \mathcal{U}_k,
  \end{equation}
  such that \( u_k = \mu_k(x_1, x_2, \ldots, x_k, u_1, \ldots, u_{k-1}) \).
\end{definition}

\begin{definition}[Markovian policy]
  A Markovian policy is defined by a sequence of functions:
  \begin{equation}
    \mu_k : \mathcal{S}_k \to \mathcal{U}_k,
  \end{equation}
  such that \( u_k = \mu_k(x_k) \).
\end{definition}

\begin{remark}[The Markov property]
  We defined the dynamical system using a deterministic function \( f_k(\cdot) \). 

  Equivalently, we could describe the evolution with the conditional probability
  \begin{equation}
    P_k(x_{k+1} | x_k, u_k) = P_s^{u}(s')
  \end{equation}
  In particular, we assume that the new state conditioned on the current state and action is not affected by the past. Formally, we assume the Markov chain induced from
  \begin{equation}
    P(x_{k+1} | x_1, \ldots, x_k, u_1, \ldots, u_k) = P_k(x_{k+1} | x_k, u_k).
  \end{equation}
\end{remark}

The MDP described above is called fully observable since the actions depend directly on the state. We will later encounter partially observable MDP where only a noisy version of the state is available to the controller.




% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Linear Systems}

\begin{definition}[Linear System]
  A linear system is given by
  \begin{equation}
      x_{t+1} = A x_t, \quad t = 0, 1, \ldots
  \end{equation}
  with some initial state \( x_0 \).

  \begin{itemize}
      \item \( x_t \in \mathbb{R}^n \) is the state vector.
      \item \( A \in \mathbb{R}^{n \times n} \) is the state-transition matrix.
  \end{itemize}
\end{definition}

\begin{lemma}[State and Decoupling in Linear Systems]
  Given a diagonalizable matrix \( A = TDT^{-1} \), the state at time \( t \) in a linear system is
  \begin{equation}
      x_t = T D^t T^{-1} x_0.
  \end{equation}
  By defining a new state \( z_t = T^{-1} x_t \), we have
  \begin{equation}
      z_t = D^t z_0,
  \end{equation}
  indicating that the states are decoupled, with each entry of \( z_t \) depending only on the corresponding entry of \( z_0 \).
\end{lemma}

\begin{remark}
  Since the eigenvalues of real matrices may be complex, we have
  \[
  \lambda = a + ib = re^{i\theta} \rightarrow \lambda^t = r^t e^{it\theta} (e^{i\theta} = \cos \theta + i \sin \theta).
  \]
  As we increase \( t \), the magnitude of \( e^{it\theta} \) is clearly unchanged. However, the length of \( r \) determines whether it converges to zero, oscillates, or blows up.
\end{remark}

\begin{definition}[Stable System]
  A system \( A \) is \textit{stable} if all of its eigenvalues have magnitude smaller than 1, i.e., \( r < 1 \).
\end{definition}






% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Linear Systems with Control}

\begin{definition}[Linear System with Control]
  A linear system with control is given by
  \begin{equation}
      x_{t+1} = A x_t + B u_t, \quad t \geq 0, \quad x_0 \in \mathbb{R}^n,
  \end{equation}
  where we added:
  \begin{itemize}
      \item \( u_t \in \mathbb{R}^m \) is the control signal (action).
      \item \( B \in \mathbb{R}^{n \times m} \) is the control matrix.
  \end{itemize}
\end{definition}

\begin{definition}[State-feedback controller]
  A controller (policy) is defined by a sequence of mappings \( \mu_t : \mathbb{R}^n \to \mathbb{R}^m \) 
  for \( t = 0, 1, \ldots, N \) such that \( u_t = \mu_t(x_t) \).  
\end{definition}
  

\begin{definition}[State-Feedback, Time-Invariant, Linear Controller]
A state-feedback, time-invariant, linear controller is any mapping of the form
\[
u_t = -K x_t.
\]
\end{definition}

\begin{definition}[Closed-Loop Matrix]
  The matrix $A_K = A - BK$ is called the closed-loop matrix of the system A. I.H.T -
  \[
    x_{t+1} = A_Kx_t = (A - BK)x_t = Ax_t + B(-Kx_t) = Ax_t + Bu_t
  \]
\end{definition}


\begin{definition}[Controllability]
  The pair \( (A, B) \) is \textit{controllable} if the system can reach any \( \xi \in \mathbb{R}^n \) from any initial state \( x_0 \in \mathbb{R}^n \) at some finite time.
\end{definition}

\begin{lemma}[Controllability Matrix]
  A pair \( (A, B) \) is controllable if and only if the \textit{controllability matrix}
  \[
  C \triangleq \begin{bmatrix} B & AB & \cdots & A^{n-1}B \end{bmatrix}
  \]
  has \( \text{rank}(C) = n \).
\end{lemma}


\begin{lemma}[Poles Placement in Controllable System]
  Controllability implies that we can choose the eigenvalues (poles) of \( A - BK \) arbitrarily.
\end{lemma}





% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{The Linear Quadratic Regulator (LQR)}

\begin{definition}[The LQR problem]
  For the linear model in (20), find a controller that minimizes
  \[
  J_N(u^N) = \sum_{i=0}^{N} [x_i^T Q x_i + u_i^T R u_i] + x_{N+1}^T Q_f x_{N+1},
  \]
  where \( u^N \triangleq u_0, u_1, \ldots, u_{N-1} \), and
  \begin{enumerate}
      \item \( Q, Q_f \succeq 0 \) are state weights
      \item \( R \succ 0 \) is the input/action/control weight. 
  \end{enumerate}
\end{definition}


\begin{lemma}[LQR matrix formulation]
  \[
\begin{bmatrix}
x_0 \\
\vdots \\
x_N
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & \cdots & 0 \\
B & 0 & \cdots & 0 \\
AB & B & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
A^{N-1}B & A^{N-2}B & \cdots & B
\end{bmatrix}
\begin{bmatrix}
u_0 \\
\vdots \\
u_{N-1}
\end{bmatrix}
+
\begin{bmatrix}
I \\
A \\
\vdots \\
A^N
\end{bmatrix}
x_0
\]
\[
\text{The matrices above can be written in short as} \quad x = Gu + Hx_0.
\]
\end{lemma}

\begin{lemma}[LQR Closed-Form Solution]
  The optimal control input \( \mathbf{u} \) that minimizes the cost function \( \mathbf{u}^T \mathbf{u} + \mathbf{x}^T \mathbf{x} \) can be achieved with
  \[
  \mathbf{u} = -(I + G^T G)^{-1} G^T H x_0.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
  \min_{\mathbf{u}} \mathbf{x}^T \mathbf{x} + \mathbf{u}^T \mathbf{u} &= \min_{\mathbf{u}} (G\mathbf{u} + H x_0)^T (G\mathbf{u} + H x_0) + \mathbf{u}^T \mathbf{u} \\
  &= \min_{\mathbf{u}} \left[ \mathbf{u}^T G^T G \mathbf{u} + 2 x_0^T H^T G \mathbf{u} + x_0^T H^T H x_0 + \mathbf{u}^T \mathbf{u} \right] \\
  &= \min_{\mathbf{u}} \left[ \mathbf{u}^T (I + G^T G) \mathbf{u} + 2 x_0^T H^T G \mathbf{u} + x_0^T H^T H x_0 \right] \\
  &= \min_{\mathbf{u}} \left[ (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0)^T (I + G^T G) (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0) \right. \\
  & \quad \left. - x_0^T H^T G (I + G^T G)^{-1} G^T H x_0 + x_0^T H^T H x_0 \right] \\
  &= \min_{\mathbf{u}} \left[ (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0)^T (I + G^T G) (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0) \right] \\
  & \quad + x_0^T H^T (I - G (I + G^T G)^{-1} G^T) H x_0 \\
  &= x_0^T H^T (I - G (I + G^T G)^{-1} G^T) H x_0 \\
  &= x_0^T H^T (I + G G^T)^{-1} H x_0,
  \end{align*}
  where the optimal control input is
  \[
  \mathbf{u} = -(I + G^T G)^{-1} G^T H x_0.
  \]
\end{proof}
      
  
\begin{remark}
  This solution is the optimal solution. However, it is not efficient since we should compute the inverse of
  \( I + G G^T \) that grows linearly with \( N \), i.e., \( O(N^3) \) computations. 
\end{remark}
  
\begin{definition}[Cost-to-Go Function]
The cost-to-go function (Value-function) is defined as
\[
V_t(z) = \min_{u_t, u_{t+1}, \ldots, u_{N-1}} \left[ \sum_{i=t}^{N-1} (x_i^T Q x_i + u_i^T R u_i) + x_N^T Q_f x_N \right]
\]
for \( x_t = z \).
\end{definition}


\begin{theorem}[Properties of the Value Function]
The value function satisfies the following properties.
\begin{enumerate}
    \item \( V_t(z) \) is a quadratic function (of the variable \( z \)). 
    
    That is, we can write \( V_t(z) = z^T P_t z \) with some \( P_t \succeq 0 \).
    
    \item The optimal controller \( u_t \) is given by
    \[
    u_t = -(R + B^T P_{t+1} B)^{-1} B^T P_{t+1} A x_t.
    \]
    \item The sequence \( P_t \) can be computed recursively with
    \[
    P_t = 
    \begin{cases} 
    Q_f & t = N \\
    Q + A^T P_{t+1} A - A^T P_{t+1} B (R + B^T P_{t+1} B)^{-1} B^T P_{t+1} A & t < N.
    \end{cases}
    \]
\end{enumerate}
\end{theorem}

Note that we can compute \( P_t \) and \( K_t \) offline prior to the control.

(Tools used in the proof - completion of the square, induction.)


\begin{definition}[Stabilizability]
The pair \( (A, B) \) is \textit{stabilizable} if
\[
\exists K \in \mathbb{R}^{m \times n} : \rho(A - BK) < 1,
\]
where \( \rho(A - BK) \) denotes the spectral radius of \( A - BK \), i.e., the largest absolute value of its eigenvalues.
\end{definition}

An equivalent characterization of stabilizability is
\[
\not\exists (x, \lambda) \ s.t. \ xA = \lambda x \ \land \ |\lambda| \ge 1 \ \land \ xB = 0.
\]
In other words, we can control the unstable modes. 

If we want \( J^*(x_0) < \infty \) for all \( x_0 \), a necessary and sufficient condition is that of stabilizability.











% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{State-space models as mappings}


\begin{definition}[Linear Time-Invariant (LTI) System]
  A discrete-time linear time-invariant (LTI) system is represented by the difference equation:
  \[
  y_k + a_1 y_{k-1} + a_2 y_{k-2} + \cdots + a_n y_{k-n} = b_0 u_k + b_1 u_{k-1} + b_2 u_{k-2} + \cdots + b_{n-1} u_{k-n+1}.
  \]
  The inputs are the \( u_i \) while the outputs are the \( y_i \).
\end{definition}



\begin{example}[Moving average]
  This system outputs the average of the current and past input values. 
  
  If \( a_1 = a_2 = \cdots = a_n = 0 \), the equation simplifies to:
  \[
  y_k = b_0 u_k + b_1 u_{k-1} + b_2 u_{k-2} + \cdots + b_{n-1} u_{k-n+1}.
  \]
  This form indicates that the output \( y_k \) is a weighted sum of the current and previous inputs, hence the term "moving average."
\end{example}
  
\begin{example}[Auto-regressive]
  This system models the output as a function of its previous values. 

  If \( b_1 = b_2 = \cdots = b_n = 0 \), the equation simplifies to:
  \[
  y_k + a_1 y_{k-1} + a_2 y_{k-2} + \cdots + a_n y_{k-n} = b_0 u_k.
  \]
  This form indicates that the output \( y_k \) depends on its past values and the current input, making it "auto-regressive."
\end{example}
  
\begin{example}[ARMA (Auto-regressive Moving Average)]
  This system combines both auto-regressive and moving average models. 
  Sometimes used as ARMA(\(i,j\)) to include the order, it captures dependencies on both past outputs and past inputs:
  \[
  y_k + a_1 y_{k-1} + a_2 y_{k-2} + \cdots + a_n y_{k-n} = b_0 u_k + b_1 u_{k-1} + b_2 u_{k-2} + \cdots + b_{n-1} u_{k-n+1}.
  \]
  Here, the output \( y_k \) is influenced by both its previous values (auto-regressive part) and previous inputs (moving average part).
\end{example}


\begin{definition}[Transfer Function \( H(z) \)]
  The transfer function \( H(z) \) of a discrete-time linear time-invariant (LTI) system represents the relationship between the Z-transform of the output \( Y(z) \) and the Z-transform of the input \( U(z) \). It provides a way to analyze the system's behavior in the frequency domain and is defined as:
  \[
  H(z) = \frac{Y(z)}{U(z)}.
  \]

  For an LTI system, this can be expressed as:
  \[
  H(z) = \frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots + b_{n-1} z^{-n+1}}{1 + a_1 z^{-1} + a_2 z^{-2} + \cdots + a_n z^{-n}}.
  \]
\end{definition}


\subsection{Controllable Canonical Form}

\begin{definition}[Controllable Canonical Form]
For the discrete-time transfer function $H(z)$, the state-space representation in controllable canonical form is given by the equations:
\[
\dot{x}(t) = A_c x(t) + B_c u(t),
\]
\[
y(t) = C_c x(t) + D_c u(t),
\]
where the state vector \( x(t) \) and input \( u(t) \) are defined as:
\[
x(t) = \begin{bmatrix}
z_1(t) \\
z_2(t) \\
\vdots \\
z_{n-1}(t) \\
z_n(t)
\end{bmatrix},
\quad
u(t) = z_n(t),
\]
and the matrices are defined as follows:
\[
A_c = \begin{bmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
-a_n & -a_{n-1} & -a_{n-2} & \cdots & -a_1
\end{bmatrix},
\quad
B_c = \begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
1
\end{bmatrix},
\]
\[
C_c = \begin{bmatrix}
b_{n-1} & b_{n-2} & \cdots & b_1 & b_0
\end{bmatrix},
\quad
D_c = 0.
\]
\end{definition}

\todo[inline]{Add the proof of the controllable canonical form + observable canonical form + Jordan canonical form.}













% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{The Asymptotic Observer}

\begin{definition}[Linear System State Estimation]
  The setting is given by a linear system:
  \[
  x_{i+1} = A x_i + B u_i, \quad x_0
  \]
  \[
  y_i = C x_i
  \]
  where:
  \begin{enumerate}
      \item \( x_i \) is the state sequence.
      \item \( u_i \) is the control sequence (which is available to us).
      \item \( y_i \) is the measurement process that we observe.
  \end{enumerate}
  
  The objective is to estimate the states. 
\end{definition}

\begin{claim}
  The optimal closed-loop estimate for a linear system is of the form 
  \[
    \hat{x}_{i+1} = A \hat{x}_i + B u_i + K(y_i - C \hat{x}_i), \quad \hat{x}_0.
  \] 
\end{claim}

\begin{claim}[Dynamics of the Estimation Error]
  The estimation error \( \tilde{x}_{i} \eqdef x_{i} - \hat{x}_{i} \) evolves according to
  \[
  \tilde{x}_{i} = (A - KC)^{i} \tilde{x}_0.
  \]
\end{claim}

\begin{proof}
  We have
  \[
  \tilde{x}_{i+1} = x_{i+1} - \hat{x}_{i+1} = A x_i + B u_i - A \hat{x}_i - B u_i - K(y_i - C \hat{x}_i) = A \tilde{x}_i - K C \tilde{x}_i = (A - KC) \tilde{x}_i.
  \]
  By induction, we conclude the claim.
\end{proof}

























% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Kalman Filter}


\begin{definition}[Kalman Filter State Space] 
  \begin{equation}
    \begin{aligned}
      x_{i+1} &= \tred{F} x_i + \tred{G} w_i \\
      y_i &= \tred{H} x_i + v_i
    \end{aligned}
  \end{equation}

  where:
\begin{itemize}
    \item \( x_{i+1} \) is the state vector at time \( i+1 \),
    \item \( x_i \) is the state vector at time \( i \),
    \item \( y_i \) is the measurement vector at time \( i \),
    \item \( w_i \) is the process noise (zero mean, uncorrelated),
    \item \( v_i \) is the measurement noise (zero mean, uncorrelated).
\end{itemize}
\end{definition}


\begin{definition}[Kalman Filter Covariance Matrix] 
  Formally, the following covariance matrix describes the model:
  \begin{equation}
      \mathbb{E} \left[
      \begin{pmatrix}
          w_i \\
          v_i \\
          x_0
      \end{pmatrix}
      \begin{pmatrix}
          w_j^* & v_j^* & x_0^* & 1
      \end{pmatrix}
      \right] = 
      \begin{pmatrix}
          \begin{pmatrix}
            \tblu{Q} & \tblu{S} \\
            \tblu{S^*} & \tblu{R}
          \end{pmatrix} \delta_{ij} & 0 & 0 \\
          0 & \tblu{\Pi_0} & 0
      \end{pmatrix},
      \label{eq:covariance_matrix}
  \end{equation}
  where \(
    \begin{pmatrix}
      \tblu{Q} & \tblu{S} \\
      \tblu{S^*} & \tblu{R}
    \end{pmatrix}
  \) and \(\tblu{\Pi_0}\) are positive semidefinite matrices and \(\delta_{ij}\) equals 1 if \(i = j\) and is zero otherwise.
  
  Note that \(w_i\) is uncorrelated as a process over time but its coordinates at a fixed time can be correlated via \(Q\).
\end{definition}

\textbf{Markings:}
\begin{itemize}
    \item \tpur{\( P_i \)} - The error covariance matrix at time \( i \)
    \begin{equation}
        \tpur{P_i} \eqdef (x_i - \hat{x}_i)(x_i - \hat{x}_i)^T
    \end{equation}

    \item \tpur{\( R_{e,i} \)} - The covariance of the innovation (or residual) at time \( i \)
    \begin{equation}
        \tpur{R_{e,i}} \eqdef \tred{H} \tpur{P_i} \tred{H^*} + \tblu{R}  
    \end{equation}

    \item \(\tpur{K_{p,i}}\) - The optimal Kalman gain at time \( i \)
    \begin{equation}
        \tpur{K_{p,i}} \triangleq (\tred{F} \tpur{P_i} \tred{H^*} + \tred{G}\tblu{S}) \tpur{R_{e,i}}^{-1}
    \end{equation}

\end{itemize}

\subsection*{Kalman Filter Optimality}

We suggest the following predictor:
\begin{equation}
    \hat{x}_{i+1|i} = \tred{F} \hat{x}_{i|i-1} + \tpur{K_{p,i}} (y_i - \tred{H} \hat{x}_{i|i-1})
\end{equation}

\begin{lemma}
  \begin{equation}
    \tilde{x}_{i+1} = (F - K_{p,i} H) \tilde{x}_i + (G - K_{p,i}) 
    \begin{pmatrix}
    w_i \\
    v_i
    \end{pmatrix}.
    \end{equation}
\end{lemma}

\begin{proof}
\begin{align*}
\tilde{x}_{i+1} &= x_{i+1} - \hat{x}_{i+1|i} \\
&= (F x_i + G w_i) - \left( F \hat{x}_i + K_{p,i} (y_i - H \hat{x}_i) \right) \\
&= F x_i + G w_i - F \hat{x}_i - K_{p,i} (H x_i + v_i - H \hat{x}_i) \\
&= F x_i + G w_i - F \hat{x}_i - K_{p,i} H x_i - K_{p,i} v_i + K_{p,i} H \hat{x}_i \\
&= F x_i - F \hat{x}_i - K_{p,i} H x_i + K_{p,i} H \hat{x}_i + G w_i - K_{p_i} v_i \\
&= (F - K_{p,i} H)(x_i - \hat{x}_i) + G w_i - K_{p,i} v_i \\
&= (F - K_{p,i} H) \tilde{x}_i + G w_i - K_{p,i} v_i.
\end{align*}
\end{proof}


\begin{lemma}
  For \( j < i \), the recursion can be evolved as
  \begin{align*}
      \tilde{x}_i &= (F - K_{p,i-1}H) \tilde{x}_{i-1} + (G - K_{p,i-1})
      \begin{pmatrix}
          w_{i-1} \\
          v_{i-1}
      \end{pmatrix} \\
      &= \ldots \\
      &= \phi_p(i, j) \tilde{x}_j + \xi_i(j),
  \end{align*}
  where
  \begin{align*}
      \phi_p(i, j) &= \prod_{k=j}^{i-1} (F - K_{p,k}H), \\
      \xi_i(j) &= \sum_{k=j}^{i-1} \phi_p(i, k+1) (G w_k - K_{p,k} v_k).
  \end{align*}
  \end{lemma}  

\end{document}
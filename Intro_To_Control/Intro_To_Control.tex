\documentclass[a4 paper]{article}
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,tikz,amssymb}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing} 
\usetikzlibrary{positioning}
\usepackage{float}
\usepackage{minted}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
% \usepackage[hidelinks]{hyperref}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{mathdesign}
\usepackage{float}
\usepackage{todonotes} 
\usepackage{empheq}
\usepackage{array}
\usepackage[ruled,vlined]{algorithm2e} 
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\usepackage{bookmark}

\usepackage{pgfplots} 
\pgfplotsset{compat=1.18}



\newtcolorbox{boxA}{
    fontupper = \bf,
    boxrule = 1.5pt,
    colframe = black % frame color
}


\setlength{\parindent}{0pt}
\numberwithin{equation}{section}


\newcommand\mycommfont[1]{\footnotesize\ttfamily\tblu{#1}}
\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\SetCommentSty{mycommfont}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}



\newtheoremstyle{boldStyle}%                % Name
  {}%                                     % Space above
  {}%                                     % Space below
  {\itshape}%                                     % Body font
  {}%                                     % Indent amount
  {\bfseries}%                            % Theorem head font
  {}%                                    % Punctuation after theorem head
  {\newline}                              % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%                                     % Theorem head spec (can be left empty, meaning `normal')


\theoremstyle{boldStyle}
\newtheorem{question}{Question}[section]
\newtheorem{answer}{Answer}[section]


\newtheorem*{claim*}{Claim}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\newtheorem*{example*}{Example}
\newtheorem*{examples*}{Examples}
\newtheorem*{definition*}{Definition}
\newtheorem*{question*}{Question}
\newtheorem*{answer*}{Answer}

\newtheorem{remark}{Remark}[section]


\newtheoremstyle{boldBlueStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{blueColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldBlueStyle}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{proposition}{Proposition}[section]



\newtheoremstyle{boldPurpleStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{purpleColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldPurpleStyle}
\newtheorem{theorem}{Theorem}[section]


\newtheoremstyle{boldRedStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{redColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldRedStyle}
\newtheorem{definition}{Definition}[section]


\newtheoremstyle{boldGreenStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{greenColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldGreenStyle}
\newtheorem{example}{Example}[section]


%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={Ashudeep Singh},%
pdftitle={Homework},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
% \usepackage[francais]{babel}
\usepackage{booktabs}
\input{../Latex_Utils/macros.tex}

\setlength{\parindent}{0pt}


\setcounter{tocdepth}{2} % Show only sections and subsections in the table of contents

\begin{document}
\homework{67678 - Introduction to Control with Learning}{Linear Dynamical Systems - Summary}{Spring 2024}
{Dr. Oron Sabag}{}{Hadar Tal}{}

% \maketitle
\tableofcontents


% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Mathematical Tools}


% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\subsection{Completing The Square}

\begin{lemma}[Completing the Square (scalars)]
  To complete the square for a quadratic equation of the form
  \[
  ax^2 + bx + c = 0,
  \]
  we can rewrite it as
  \[
  a(x + d)^2 + e = 0,
  \]
  where
  \[
  d = \frac{b}{2a} \quad \text{and} \quad e = c - \frac{b^2}{4a}.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    ax^2 + bx + c &= 0 &\rightarrow\quad & x^2 + \frac{b}{a}x + \frac{c}{a} = 0 &\rightarrow\quad& x^2 + 
    \frac{b}{a}x = -\frac{c}{a} \quad \rightarrow \\
    x^2 + \frac{b}{a}x + \left( \frac{b}{2a} \right)^2 &= -\frac{c}{a} + \left( \frac{b}{2a} \right)^2 
    &\rightarrow\quad& \left( x + \frac{b}{2a} \right)^2 = \frac{b^2}{4a^2} - \frac{c}{a} &\rightarrow\quad& 
    \left( x + \frac{b}{2a} \right)^2 = \frac{b^2 - 4ac}{4a^2} \quad \rightarrow \\
    a\left( x + \frac{b}{2a} \right)^2 &= a\left( \frac{b^2 - 4ac}{4a^2} \right) 
    &\rightarrow\quad& a\left( x + \frac{b}{2a} \right)^2 = \frac{b^2 - 4ac}{4a} 
    &\rightarrow\quad& a\left( x + \frac{b}{2a} \right)^2 + c - \frac{b^2}{4a} = 0
    \end{align*}
\end{proof}

\begin{lemma}[Completing the Square for Quadratic Forms]
  Given a quadratic form \( x^T A x + b^T x + c \), where \( A \) is a symmetric positive definite matrix, \( b \) is a vector, 
  and \( c \) is a scalar, the expression can be completed to a perfect square as follows:
  \begin{align*}
    x^T A x + b^T x + c &= (x + \tblu{A^{-1} b/2})^T A (x + \tblu{A^{-1} b/2}) + c \text{ } \tred{ - \frac{1}{4} b^T A^{-1} b}
  \end{align*}
\end{lemma}









% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\subsection{The Z-transform}

The Z-transform converts a discrete-time signal, which is a sequence of real or complex numbers, into a complex frequency domain representation. 

The Z-transform of a discrete-time signal \( x[k] \) is defined as:

\[
X(z) = \mathcal{Z}\{x[k]\} = \sum_{k=-\infty}^{\infty} x[k] z^{-k},
\]

where \( z \) is a complex variable. The Z-transform is particularly useful for analyzing linear time-invariant (LTI) systems.

\subsubsection{Properties of the Z-transform}
\begin{enumerate}
  \item \textbf{Linearity:}
  \[
  \mathcal{Z}\{a x[k] + b y[k]\} = a X(z) + b Y(z).
  \]

  \item \textbf{Time Shifting:}
  \[
  \mathcal{Z}\{x[k-n]\} = z^{-n} X(z).
  \]

  \item \textbf{Convolution:}
  \[
  \mathcal{Z}\{x[k] * y[k]\} = X(z) Y(z).
  \]

  \item \textbf{Initial Value Theorem:}
  \[
  x[0] = \lim_{z \to \infty} X(z).
  \]

  \item \textbf{Final Value Theorem:}
  \[
  \lim_{k \to \infty} x[k] = \lim_{z \to 1} (1 - z^{-1}) X(z),
  \]
  provided the limits exist.
\end{enumerate}



\subsubsection{Differences Between Z-transform and Laplace Transform}

\begin{enumerate}
  \item \textbf{Domain:}
  The Z-transform is used for discrete-time signals, while the Laplace transform is used for continuous-time signals.

  \item \textbf{Definition:}
  The Z-transform is defined as a summation:
  \[
  X(z) = \sum_{k=-\infty}^{\infty} x[k] z^{-k},
  \]
  while the Laplace transform is defined as an integral:
  \[
  X(s) = \int_{0}^{\infty} x(t) e^{-st} \, dt.
  \]

  \item \textbf{Complex Variable:}
  The Z-transform uses the complex variable \( z \), typically represented as \( z = e^{sT} \) where \( T \) is the sampling period. The Laplace transform uses the complex variable \( s \).

  \item \textbf{Application:}
  The Z-transform is applied to discrete-time systems and signals, making it useful for digital signal processing and discrete control systems. The Laplace transform is applied to continuous-time systems and signals, making it useful for analog signal processing and continuous control systems.
\end{enumerate}











% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\subsection{UDL and LDU Factorizations}


\begin{lemma}[UDL Decomposition]
  If \( A \) and \( D \) are square matrices, and \( D \) is invertible, we can write:
  \[
  \begin{pmatrix}
  A & B \\
  C & D
  \end{pmatrix} =
  \begin{pmatrix}
  I & BD^{-1} \\
  0 & I
  \end{pmatrix}
  \begin{pmatrix}
  \Delta_D & 0 \\
  0 & D
  \end{pmatrix}
  \begin{pmatrix}
  I & 0 \\
  D^{-1}C & I
  \end{pmatrix},
  \]
  where \( \Delta_D = A - BD^{-1}C \) is the \tred{Schur complement} of the matrix.
\end{lemma}

\begin{example}[UDL Decomposition Example]
  Consider the scalars:
  \[
  A = 8, \quad B = 6, \quad C = 4, \quad D = 2.
  \]
  Since \( D \) is invertible, we apply the UDL decomposition:
  \[
  \Delta_D = A - BD^{-1}C = 8 - 6 \cdot \frac{1}{2} \cdot 4 = 8 - 12 = -4.
  \]
  Thus, the decomposition is:
  \[
  \begin{pmatrix}
  8 & 6 \\
  4 & 2
  \end{pmatrix} =
  \begin{pmatrix}
  1 & 3 \\
  0 & 1
  \end{pmatrix}
  \begin{pmatrix}
  -4 & 0 \\
  0 & 2
  \end{pmatrix}
  \begin{pmatrix}
  1 & 0 \\
  2 & 1
  \end{pmatrix}.
  \]
\end{example}




\bigbreak



% \begin{lemma}[LDU Decomposition]
%   If \( A \) and \( D \) are square matrices, and \( A \) is invertible, we can write:
%   \[
%   \begin{pmatrix}
%   A & B \\
%   C & D
%   \end{pmatrix} =
%   \begin{pmatrix}
%   I & 0 \\
%   CA^{-1} & I
%   \end{pmatrix}
%   \begin{pmatrix}
%   A & 0 \\
%   0 & \Delta_A
%   \end{pmatrix}
%   \begin{pmatrix}
%   I & A^{-1}B \\
%   0 & I
%   \end{pmatrix},
%   \]
%   where \( \Delta_A = D - CA^{-1}B \) is the \tred{Schur complement} of the matrix.
% \end{lemma}


% \begin{example}[LDU Decomposition Example]
%   Consider the scalars:
%   \[
%   A = 8, \quad B = 6, \quad C = 4, \quad D = 2.
%   \]
%   Since \( A \) is invertible, we apply the LDU decomposition:
%   \[
%   \Delta_A = D - CA^{-1}B = 2 - 4 \cdot \frac{1}{8} \cdot 6 = 2 - 3 = -1.
%   \]
%   Thus, the decomposition is:
%   \[
%   \begin{pmatrix}
%   8 & 6 \\
%   4 & 2
%   \end{pmatrix} =
%   \begin{pmatrix}
%   1 & 0 \\
%   \frac{1}{2} & 1
%   \end{pmatrix}
%   \begin{pmatrix}
%   8 & 0 \\
%   0 & -1
%   \end{pmatrix}
%   \begin{pmatrix}
%   1 & \frac{3}{4} \\
%   0 & 1
%   \end{pmatrix}.
%   \]
%   \end{example}














% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\subsection{Least Squares}

\subsubsection{Definition}

\begin{definition}[Least Squares Problem]
For a set of linear equations
\[
y = Hx,
\]
where \(x \in \mathbb{R}^m\) is the unknown state vector, \( y \in \mathbb{R}^n \) is the 
measurements vector and \( H \in \mathbb{R}^{n \times m} \),

The least squares solution \(\hat{x}^o\) is defined as 
\[
\hat{x}^o = \argmin_{\hat{x} \in \mathbb{R}^m} J(\hat{x}) = \argmin_{\hat{x} \in \mathbb{R}^m} \|y - H\hat{x}\|^2 = 
\argmin_{\hat{x} \in \mathbb{R}^m} (y - H\hat{x})^T (y - H\hat{x}).
\]

\end{definition}

\begin{lemma}
A solution \( \hat{x} \) is optimal, i.e., \( J(\hat{x}) \leq J(x) \) for all \( x \) if the normal equations
\[
H^* H \hat{x} = H^* y
\]
are satisfied. The optimal objective is
\[
J(\hat{x}) = \|y\|^2 - \|H \hat{x}\|^2.
\]
\end{lemma}


\begin{remark}
  The optimal estimate satisfies the orthogonality principle: \( \|y\|^2 =  \|y - H \hat{x}\|^2  + \|H \hat{x}\|^2 \)
\end{remark}



\subsubsection{Stochastic Least Squares}


\begin{definition}[Stochastic Least Squares]
  Given two random vectors \( X \) and \( Y \) with a probability density function \( f_{X,Y} \), 
  the objective is to construct an estimator for \( X \) given \( Y \), denoted by \( \hat{X}(Y) \). 
  
  The error \( \Delta = X - \hat{X}(Y) \) is a random variable on its own, so we study the mean squared error (MSE):
  \[
  MSE = \mathbb{E}[(X - \hat{X}(Y))^T (X - \hat{X}(Y))].
  \]
\end{definition}

\begin{lemma}
The estimator that minimizes the MSE is the conditional expectation:
\[
\hat{X} = \mathbb{E}[X | Y].
\]

In other words, for any \( g : \mathcal{Y} \rightarrow \mathcal{X} \), we have
\[
\mathbb{E}[(X - g(Y))^T (X - g(Y))] \geq \mathbb{E}[(X - \mathbb{E}[X | Y])^T (X - \mathbb{E}[X | Y])].
\]
\end{lemma}



\begin{definition}[Linear Least Mean Square Estimator (LLMSE)]
  A linear estimator takes the form \( \hat{X} = K Y \), where \( K \) is a matrix to be optimized. 
  
  The \tred{error covariance matrix of a linear estimator} \( K \) is defined as:
  \[
  P(K) = \mathbb{E}[(X - K Y)(X - K Y)^T].
  \]
  
  We say \( K_0 \) is a linear least mean square estimator (LLMSE) if:
  \[
  P(K) \succeq P(K_0)
  \]
  for any \( K \). Alternatively, this can be written as:
  \[
  a^T P(K) a \geq a^T P(K_0) a,
  \]
  for all \( K \) and vectors \( a \). 
\end{definition}

\begin{claim}
  The LLMSE is optimal if (X,Y) are jointly Gaussian.
\end{claim}


\begin{theorem}
  Any LLMSE \( K_0 \) satisfies the normal equations:
  \[
  K_0 R_Y = R_{XY},
  \]
  where \( R_Y = \mathbb{E}[YY^T] \) is the covariance of the measurements, and \( R_{XY} \) is the covariance between \( X \) and \( Y \).
  \end{theorem}
  
  If \( R_Y \) is invertible, we obtain the well-known estimator:
  \[
  \hat{X} = R_{XY} R_Y^{-1} Y,
  \]
  with the estimation error covariance:
  \[
  P(K_0) = R_X - R_{XY} R_Y^{-1} R_{YX}.
  \]
  
  \begin{remark}
  We can gain some intuition on the proposed solution by writing the covariance matrix using a UDL factorization:
  \[
  \begin{pmatrix}
  R_X & R_{XY} \\
  R_{YX} & R_Y
  \end{pmatrix} =
  \begin{pmatrix}
  I & R_{XY} R_Y^{-1} \\
  0 & I
  \end{pmatrix}
  \begin{pmatrix}
  R_X - R_{XY} R_Y^{-1} R_{YX} & 0 \\
  0 & R_Y
  \end{pmatrix}
  \begin{pmatrix}
  I & 0 \\
  R_Y^{-1} R_{YX} & I
  \end{pmatrix}.
  \]
  
  \begin{enumerate}
      \item \textbf{Starting Point:}
      \begin{itemize}
          \item We start with the joint covariance matrix of \( X \) and \( Y \):
          \[
          \begin{pmatrix}
          R_X & R_{XY} \\
          R_{YX} & R_Y
          \end{pmatrix}.
          \]
      \end{itemize}
      
      \item \textbf{UDL Factorization:}
      \begin{itemize}
          \item We apply the UDL factorization:
          \[
          \begin{pmatrix}
          R_X & R_{XY} \\
          R_{YX} & R_Y
          \end{pmatrix} =
          \begin{pmatrix}
          I & R_{XY} R_Y^{-1} \\
          0 & I
          \end{pmatrix}
          \begin{pmatrix}
          R_X - R_{XY} R_Y^{-1} R_{YX} & 0 \\
          0 & R_Y
          \end{pmatrix}
          \begin{pmatrix}
          I & 0 \\
          R_Y^{-1} R_{YX} & I
          \end{pmatrix}.
          \]
      \end{itemize}
      
      \item \textbf{Explanation of Terms:}
      \begin{itemize}
          \item \( R_X - R_{XY} R_Y^{-1} R_{YX} \): This term represents the optimal error variance.
          \item The matrix \( \begin{pmatrix} I & R_{XY} R_Y^{-1} \\ 0 & I \end{pmatrix} \) projects the variable \( X \) onto the linear space spanned by \( Y \).
      \end{itemize}
      
      \item \textbf{Optimal Error Variance:}
      \begin{itemize}
          \item The first element of the diagonal matrix is the optimal error variance:
          \[
          R_X - R_{XY} R_Y^{-1} R_{YX}.
          \]
          \item Since this term appears in the factorization, it confirms that the estimator minimizes the estimation error.
      \end{itemize}
      
      \item \textbf{Estimation:}
      \begin{itemize}
          \item We can express the relationship between \( X \) and \( \tilde{X} \) using the projection matrix:
          \[
          \begin{pmatrix}
          X \\
          Y
          \end{pmatrix} =
          \begin{pmatrix}
          I & R_{XY} R_Y^{-1} \\
          0 & I
          \end{pmatrix}
          \begin{pmatrix}
          \tilde{X} \\
          Y
          \end{pmatrix}.
          \]
          \item This equation shows that \( \hat{X} \) is the projection of \( X \) onto the space spanned by \( Y \).
      \end{itemize}
      
      \item \textbf{Projection Interpretation:}
      \begin{itemize}
          \item This can be viewed as a Gram-Schmidt process to project the variable \( X \) onto the linear space spanned by \( Y \) with the inner product \( \langle X, Y \rangle = \mathbb{E}[XY^T] \). Thus, we will sometimes write it as:
          \[
          \hat{X} = \langle X, Y \rangle \langle Y, Y \rangle^{-1} Y = \langle X, Y \rangle \|Y\|^{-2} Y.
          \]
      \end{itemize}
  \end{enumerate}
  
  By using the UDL factorization and interpreting the projection, we understand how the LLMSE minimizes the estimation error variance.
  \end{remark}


\begin{theorem}[Sum of Predictions]
  Given two independent samples \( Y_1 \) and \( Y_2 \), the optimal predictor for \( X \) based on \( Y = (Y_1, Y_2) \) is the sum of the predictors based on \( Y_1 \) and \( Y_2 \) separately. Mathematically, if the samples \( Y_1 \) and \( Y_2 \) are independent, then:
  \[
  \hat{X}|_{Y_1, Y_2} = \hat{X}|_{Y_1} + \hat{X}|_{Y_2} \iff \langle Y_1, Y_2 \rangle = 0.
  \]
\end{theorem}
  
\begin{proof}
  \begin{align*}
    \hat{X}|_{Y_1, Y_2} &= R_{XY} R_Y^{-1} Y \\
    &= R_{XY} 
    \begin{pmatrix}
    R_{Y_1} & R_{Y_1 Y_2} \\
    R_{Y_2 Y_1} & R_{Y_2}
    \end{pmatrix}^{-1}
    \begin{pmatrix}
    Y_1 \\
    Y_2
    \end{pmatrix} \\
    &= R_{XY} 
    \begin{pmatrix}
    R_{Y_1} & 0 \\
    0 & R_{Y_2}
    \end{pmatrix}^{-1}
    \begin{pmatrix}
    Y_1 \\
    Y_2
    \end{pmatrix} \quad (\text{independence of } Y_1 \text{ and } Y_2) \\
    &= R_{XY}
    \begin{pmatrix}
    R_{Y_1}^{-1} & 0 \\
    0 & R_{Y_2}^{-1}
    \end{pmatrix}
    \begin{pmatrix}
    Y_1 \\
    Y_2
    \end{pmatrix} \\
    &= R_{XY}
    \begin{pmatrix}
    R_{Y_1}^{-1} Y_1 \\
    R_{Y_2}^{-1} Y_2
    \end{pmatrix} \\
    &= \begin{pmatrix}
    R_{XY_1} & R_{XY_2}
    \end{pmatrix}
    \begin{pmatrix}
    R_{Y_1}^{-1} Y_1 \\
    R_{Y_2}^{-1} Y_2
    \end{pmatrix} \\
    &= R_{XY_1} R_{Y_1}^{-1} Y_1 + R_{XY_2} R_{Y_2}^{-1} Y_2 \\
    &= \hat{X}|_{Y_1} + \hat{X}|_{Y_2}
  \end{align*}
\end{proof}







% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{The Shortest Path Problem}

\begin{definition}[The Shortest Path Problem]
  Given a directed graph \( G = (V, E) \), each edge in the graph has a cost \( a^t_{ij} \) where 
  \( i \) is the outgoing node, \( j \) is the node to which the edge is connected, 
  and \( t \in \{0, \ldots, N+1\} \) refers to the time. We adopt the convention that no edge implies 
  an infinite cost \( a^t_{ij} = \infty \).

  The objective is to minimize the cumulative cost on a path from the source node \( S_0 = S \) to the 
  terminal node \( S_{N+1} = T \). Formally, we aim to solve the optimization
  \begin{equation}
      J^* = \min_{\{n_i \in \mathcal{S}_i\}_{i=0}^{N+1}} \sum_{t=0}^{N} a^t_{n_t, n_{t+1}}.
  \end{equation}
\end{definition}

\begin{definition}[Cost-to-Go Function]
  We define \( J_k(i) \) as the cost-to-go function corresponding to the minimal cost from time \( k \) 
  until the end when starting at node \( i \). Formally, for \( k = 0, \ldots, N \), define
  \begin{equation}
      J_k(i) = \min_{\{n_j \in \mathcal{S}_j | j=k+1,\ldots,N,n_k=i\}} \sum_{j=k}^{N} a^j_{n_t, n_{t+1}}, 
      \quad \forall i \in \mathcal{S}_k.
  \end{equation}
\end{definition}


\begin{algorithm}[H]
    \SetNoFillComment
    \SetAlgoLined
    \KwIn{Cost matrix \( a^t_{ij} \) and nodes \( \mathcal{S}_0, \mathcal{S}_1, \ldots, \mathcal{S}_{N+1} \)}
    \KwOut{Cost-to-go functions \( J_k(i) \)}
    Initialize \( J_N(i) = a^N_{iT} \) \;
    \For{$k = N-1, \ldots, 0$}{
        \For{$i \in \mathcal{S}_k$}{
            $J_k(i) = \min_{j \in \mathcal{S}_{k+1}} \left[ a^k_{ij} + J_{k+1}(j) \right]$
        }
    }
    \caption{\tpur{Dynamic Programming Solution for the Shortest Path Problem (Cost-to-Go)}} \label{algo:dp_solution_shortest_path}
\end{algorithm}


\begin{definition}[Cost-to-Arrive Function]
  We define \( J_{N-k}(j) \) as the cost-to-arrive function corresponding to the minimal cost from time \( 1 \) 
  until time \( k \) when arriving at node \( j \). Formally, for \( k = 0, \ldots, N \), define
  \begin{equation}
      J_{N-k}(j) = \min_{\{n_i \in \mathcal{S}_i | i=1,\ldots,k-1,n_k=j\}} \sum_{i=1}^{k} a^i_{n_{i-1}, n_i}, 
      \quad \forall j \in \mathcal{S}_k.
  \end{equation}
\end{definition}

\begin{algorithm}[H]
    \SetNoFillComment
    \SetAlgoLined
    \KwIn{Cost matrix \( a^t_{ij} \) and nodes \( \mathcal{S}_0, \mathcal{S}_1, \ldots, \mathcal{S}_{N+1} \)}
    \KwOut{Cost-to-arrive functions \( J_{N-k}(j) \)}
    Initialize \( J_N(j) = a^0_{sj} \), \( \forall j \in \mathcal{S}_1 \) \;
    \For{$k = 1, \ldots, N$}{
        \For{$j \in \mathcal{S}_{N-k+1}$}{
            $J_k(j) = \min_{i \in \mathcal{S}_{N-k}} \left[ a^{N-k}_{ij} + J_{k+1}(i) \right]$
        }
    }
    \caption{\tpur{Forward Algorithm for the Shortest Path Problem (Cost-to-Arrive)}} \label{algo:forward_algorithm_shortest_path}
\end{algorithm}



% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Markov Decision Processes (MDPs)}

\begin{definition}[MDP]
  An MDP is defined by the following elements:
  \begin{enumerate}
    \item The state at time \( k \) is \( x_k \) and takes values in the set \( \mathcal{S}_k \).
    \item The action at time \( k \) is \( u_k \) and takes values from \( \mathcal{U}_k \).
    \item The disturbance at time \( k \) is \( w_k \) and takes values from \( \mathcal{W}_k \).
    \item A dynamical system is given by the function
      \begin{equation}
        x_{k+1} = f_k(x_k, u_k, w_k), \quad k = 0, \ldots, N-1.
      \end{equation}
    \item The probabilistic law of the disturbance random variable \( w_k \) is characterized by \( P_{W_k}(\cdot | x_k, u_k) \) conditioned on the state \( x_k \) and the action \( u_k \).
    \item A cost function \( g_k : \mathcal{S}_k \times \mathcal{U}_k \to \mathbb{R} \).
  \end{enumerate}
\end{definition}

The cost over a horizon \( N \) is
\begin{equation}
  S_\pi(x_0) = \mathbb{E}[g_N(x_N) + \sum_{k=0}^{N-1} g_k(x_k, u_k)],
\end{equation}
where \( g_N(\cdot) \) is the terminal cost.

\begin{definition}[History-dependent policy]
  A history-dependent policy is defined by a sequence of functions:
  \begin{equation}
    \mu_k : \mathcal{S}_1 \times \cdots \times \mathcal{S}_k \times \mathcal{U}_1 \times \cdots \mathcal{U}_{k-1} \to \mathcal{U}_k,
  \end{equation}
  such that \( u_k = \mu_k(x_1, x_2, \ldots, x_k, u_1, \ldots, u_{k-1}) \).
\end{definition}

\begin{definition}[Markovian policy]
  A Markovian policy is defined by a sequence of functions:
  \begin{equation}
    \mu_k : \mathcal{S}_k \to \mathcal{U}_k,
  \end{equation}
  such that \( u_k = \mu_k(x_k) \).
\end{definition}

\begin{remark}[The Markov property]
  We defined the dynamical system using a deterministic function \( f_k(\cdot) \). 

  Equivalently, we could describe the evolution with the conditional probability
  \begin{equation}
    P_k(x_{k+1} | x_k, u_k) = P_s^{u}(s')
  \end{equation}
  In particular, we assume that the new state conditioned on the current state and action is not affected by the past. Formally, we assume the Markov chain induced from
  \begin{equation}
    P(x_{k+1} | x_1, \ldots, x_k, u_1, \ldots, u_k) = P_k(x_{k+1} | x_k, u_k).
  \end{equation}
\end{remark}

The MDP described above is called fully observable since the actions depend directly on the state. We will later encounter partially observable MDP where only a noisy version of the state is available to the controller.




% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Linear Systems}

\begin{definition}[Linear System]
  A linear system is given by
  \begin{equation}
      x_{t+1} = A x_t, \quad t = 0, 1, \ldots
  \end{equation}
  with some initial state \( x_0 \).

  \begin{itemize}
      \item \( x_t \in \mathbb{R}^n \) is the state vector.
      \item \( A \in \mathbb{R}^{n \times n} \) is the state-transition matrix.
  \end{itemize}
\end{definition}

\begin{lemma}[State and Decoupling in Linear Systems]
  Given a diagonalizable matrix \( A = TDT^{-1} \), the state at time \( t \) in a linear system is
  \begin{equation}
      x_t = T D^t T^{-1} x_0.
  \end{equation}
  By defining a new state \( z_t = T^{-1} x_t \), we have
  \begin{equation}
      z_t = D^t z_0,
  \end{equation}
  indicating that the states are decoupled, with each entry of \( z_t \) depending only on the corresponding entry of \( z_0 \).
\end{lemma}

\begin{remark}
  Since the eigenvalues of real matrices may be complex, we have
  \[
  \lambda = a + ib = re^{i\theta} \rightarrow \lambda^t = r^t e^{it\theta} (e^{i\theta} = \cos \theta + i \sin \theta).
  \]
  As we increase \( t \), the magnitude of \( e^{it\theta} \) is clearly unchanged. However, the length of \( r \) determines whether it converges to zero, oscillates, or blows up.
\end{remark}

\begin{definition}[Stable System]
  A system \( A \) is \textit{stable} if all of its eigenvalues have magnitude smaller than 1, i.e., \( r < 1 \).
\end{definition}






% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Linear Systems with Control}

\begin{definition}[Linear System with Control]
  A linear system with control is given by
  \begin{equation}
      x_{t+1} = A x_t + B u_t, \quad t \geq 0, \quad x_0 \in \mathbb{R}^n,
  \end{equation}
  where we added:
  \begin{itemize}
      \item \( u_t \in \mathbb{R}^m \) is the control signal (action).
      \item \( B \in \mathbb{R}^{n \times m} \) is the control matrix.
  \end{itemize}
\end{definition}

\begin{definition}[State-feedback controller]
  A controller (policy) is defined by a sequence of mappings \( \mu_t : \mathbb{R}^n \to \mathbb{R}^m \) 
  for \( t = 0, 1, \ldots, N \) such that \( u_t = \mu_t(x_t) \).  
\end{definition}
  

\begin{definition}[State-Feedback, Time-Invariant, Linear Controller]
A state-feedback, time-invariant, linear controller is any mapping of the form
\[
u_t = -K x_t.
\]
\end{definition}

\begin{definition}[Closed-Loop Matrix]
  The matrix $A_K = A - BK$ is called the closed-loop matrix of the system A. I.H.T -
  \[
    x_{t+1} = A_Kx_t = (A - BK)x_t = Ax_t + B(-Kx_t) = Ax_t + Bu_t
  \]
\end{definition}


\begin{definition}[Controllability]
  The pair \( (A, B) \) is \textit{controllable} if the system can reach any \( \xi \in \mathbb{R}^n \) from any initial state \( x_0 \in \mathbb{R}^n \) at some finite time.
\end{definition}

\begin{lemma}[Controllability Matrix]
  A pair \( (A, B) \) is controllable if and only if the \textit{controllability matrix}
  \[
  C \triangleq \begin{bmatrix} B & AB & \cdots & A^{n-1}B \end{bmatrix}
  \]
  has \( \text{rank}(C) = n \).
\end{lemma}


\begin{lemma}[Poles Placement in Controllable System]
  Controllability implies that we can choose the eigenvalues (poles) of \( A - BK \) arbitrarily.
\end{lemma}





% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{The Linear Quadratic Regulator (LQR)}

\begin{definition}[The LQR problem]
  For the linear model in (20), find a controller that minimizes
  \[
  J_N(u^N) = \sum_{i=0}^{N} [x_i^T Q x_i + u_i^T R u_i] + x_{N+1}^T Q_f x_{N+1},
  \]
  where \( u^N \triangleq u_0, u_1, \ldots, u_{N-1} \), and
  \begin{enumerate}
      \item \( Q, Q_f \succeq 0 \) are state weights
      \item \( R \succ 0 \) is the input/action/control weight. 
  \end{enumerate}
\end{definition}


\begin{lemma}[LQR matrix formulation]
  \[
\begin{bmatrix}
x_0 \\
\vdots \\
x_N
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & \cdots & 0 \\
B & 0 & \cdots & 0 \\
AB & B & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
A^{N-1}B & A^{N-2}B & \cdots & B
\end{bmatrix}
\begin{bmatrix}
u_0 \\
\vdots \\
u_{N-1}
\end{bmatrix}
+
\begin{bmatrix}
I \\
A \\
\vdots \\
A^N
\end{bmatrix}
x_0
\]
\[
\text{The matrices above can be written in short as} \quad x = Gu + Hx_0.
\]
\end{lemma}

\begin{lemma}[LQR Closed-Form Solution]
  The optimal control input \( \mathbf{u} \) that minimizes the cost function \( \mathbf{u}^T \mathbf{u} + \mathbf{x}^T \mathbf{x} \) can be achieved with
  \[
  \mathbf{u} = -(I + G^T G)^{-1} G^T H x_0.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
  \min_{\mathbf{u}} \mathbf{x}^T \mathbf{x} + \mathbf{u}^T \mathbf{u} &= \min_{\mathbf{u}} (G\mathbf{u} + H x_0)^T (G\mathbf{u} + H x_0) + \mathbf{u}^T \mathbf{u} \\
  &= \min_{\mathbf{u}} \left[ \mathbf{u}^T G^T G \mathbf{u} + 2 x_0^T H^T G \mathbf{u} + x_0^T H^T H x_0 + \mathbf{u}^T \mathbf{u} \right] \\
  &= \min_{\mathbf{u}} \left[ \mathbf{u}^T (I + G^T G) \mathbf{u} + 2 x_0^T H^T G \mathbf{u} + x_0^T H^T H x_0 \right] \\
  &= \min_{\mathbf{u}} \left[ (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0)^T (I + G^T G) (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0) \right. \\
  & \quad \left. - x_0^T H^T G (I + G^T G)^{-1} G^T H x_0 + x_0^T H^T H x_0 \right] \\
  &= \min_{\mathbf{u}} \left[ (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0)^T (I + G^T G) (\mathbf{u} + (I + G^T G)^{-1} G^T H x_0) \right] \\
  & \quad + x_0^T H^T (I - G (I + G^T G)^{-1} G^T) H x_0 \\
  &= x_0^T H^T (I - G (I + G^T G)^{-1} G^T) H x_0 \\
  &= x_0^T H^T (I + G G^T)^{-1} H x_0,
  \end{align*}
  where the optimal control input is
  \[
  \mathbf{u} = -(I + G^T G)^{-1} G^T H x_0.
  \]
\end{proof}
      
  
\begin{remark}
  This solution is the optimal solution. However, it is not efficient since we should compute the inverse of
  \( I + G G^T \) that grows linearly with \( N \), i.e., \( O(N^3) \) computations. 
\end{remark}
  
\begin{definition}[Cost-to-Go Function]
The cost-to-go function (Value-function) is defined as
\[
V_t(z) = \min_{u_t, u_{t+1}, \ldots, u_{N-1}} \left[ \sum_{i=t}^{N-1} (x_i^T Q x_i + u_i^T R u_i) + x_N^T Q_f x_N \right]
\]
for \( x_t = z \).
\end{definition}


\begin{theorem}[Properties of the Value Function]
The value function satisfies the following properties.
\begin{enumerate}
    \item \( V_t(z) \) is a quadratic function (of the variable \( z \)). 
    
    That is, we can write \( V_t(z) = z^T P_t z \) with some \( P_t \succeq 0 \).
    
    \item The optimal controller \( u_t \) is given by
    \[
    u_t = -(R + B^T P_{t+1} B)^{-1} B^T P_{t+1} A x_t.
    \]
    \item The sequence \( P_t \) can be computed recursively with
    \[
    P_t = 
    \begin{cases} 
    Q_f & t = N \\
    Q + A^T P_{t+1} A - A^T P_{t+1} B (R + B^T P_{t+1} B)^{-1} B^T P_{t+1} A & t < N.
    \end{cases}
    \]
\end{enumerate}
\end{theorem}

Note that we can compute \( P_t \) and \( K_t \) offline prior to the control.

(Tools used in the proof - completion of the square, induction.)


\begin{definition}[Stabilizability]
The pair \( (A, B) \) is \textit{stabilizable} if
\[
\exists K \in \mathbb{R}^{m \times n} : \rho(A - BK) < 1,
\]
where \( \rho(A - BK) \) denotes the spectral radius of \( A - BK \), i.e., the largest absolute value of its eigenvalues.
\end{definition}

An equivalent characterization of stabilizability is
\[
\not\exists (x, \lambda) \ s.t. \ xA = \lambda x \ \land \ |\lambda| \ge 1 \ \land \ xB = 0.
\]
In other words, we can control the unstable modes. 

If we want \( J^*(x_0) < \infty \) for all \( x_0 \), a necessary and sufficient condition is that of stabilizability.











% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{State-space models as mappings}


\begin{definition}[Linear Time-Invariant (LTI) System]
  A discrete-time linear time-invariant (LTI) system is represented by the difference equation:
  \[
  y_k + a_1 y_{k-1} + a_2 y_{k-2} + \cdots + a_n y_{k-n} = b_0 u_k + b_1 u_{k-1} + b_2 u_{k-2} + \cdots + b_{n-1} u_{k-n+1}.
  \]
  The inputs are the \( u_i \) while the outputs are the \( y_i \).
\end{definition}



\begin{example}[Moving average]
  This system outputs the average of the current and past input values. 
  
  If \( a_1 = a_2 = \cdots = a_n = 0 \), the equation simplifies to:
  \[
  y_k = b_0 u_k + b_1 u_{k-1} + b_2 u_{k-2} + \cdots + b_{n-1} u_{k-n+1}.
  \]
  This form indicates that the output \( y_k \) is a weighted sum of the current and previous inputs, hence the term "moving average."
\end{example}
  
\begin{example}[Auto-regressive]
  This system models the output as a function of its previous values. 

  If \( b_1 = b_2 = \cdots = b_n = 0 \), the equation simplifies to:
  \[
  y_k + a_1 y_{k-1} + a_2 y_{k-2} + \cdots + a_n y_{k-n} = b_0 u_k.
  \]
  This form indicates that the output \( y_k \) depends on its past values and the current input, making it "auto-regressive."
\end{example}
  
\begin{example}[ARMA (Auto-regressive Moving Average)]
  This system combines both auto-regressive and moving average models. 
  Sometimes used as ARMA(\(i,j\)) to include the order, it captures dependencies on both past outputs and past inputs:
  \[
  y_k + a_1 y_{k-1} + a_2 y_{k-2} + \cdots + a_n y_{k-n} = b_0 u_k + b_1 u_{k-1} + b_2 u_{k-2} + \cdots + b_{n-1} u_{k-n+1}.
  \]
  Here, the output \( y_k \) is influenced by both its previous values (auto-regressive part) and previous inputs (moving average part).
\end{example}


\begin{definition}[Transfer Function \( H(z) \)]
  The transfer function \( H(z) \) of a discrete-time linear time-invariant (LTI) system represents the relationship between the Z-transform of the output \( Y(z) \) and the Z-transform of the input \( U(z) \). It provides a way to analyze the system's behavior in the frequency domain and is defined as:
  \[
  H(z) = \frac{Y(z)}{U(z)}.
  \]

  For an LTI system, this can be expressed as:
  \[
  H(z) = \frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \cdots + b_{n-1} z^{-n+1}}{1 + a_1 z^{-1} + a_2 z^{-2} + \cdots + a_n z^{-n}}.
  \]
\end{definition}


\subsection{Controllable Canonical Form}

\begin{definition}[Controllable Canonical Form]
For the discrete-time transfer function $H(z)$, the state-space representation in controllable canonical form is given by the equations:
\[
\dot{x}(t) = A_c x(t) + B_c u(t),
\]
\[
y(t) = C_c x(t) + D_c u(t),
\]
where the state vector \( x(t) \) and input \( u(t) \) are defined as:
\[
x(t) = \begin{bmatrix}
z_1(t) \\
z_2(t) \\
\vdots \\
z_{n-1}(t) \\
z_n(t)
\end{bmatrix},
\quad
u(t) = z_n(t),
\]
and the matrices are defined as follows:
\[
A_c = \begin{bmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
-a_n & -a_{n-1} & -a_{n-2} & \cdots & -a_1
\end{bmatrix},
\quad
B_c = \begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
1
\end{bmatrix},
\]
\[
C_c = \begin{bmatrix}
b_{n-1} & b_{n-2} & \cdots & b_1 & b_0
\end{bmatrix},
\quad
D_c = 0.
\]
\end{definition}

\todo[inline]{Add the proof of the controllable canonical form + observable canonical form + Jordan canonical form.}













% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{The Asymptotic Observer}

\begin{definition}[Linear System State Estimation]
  The setting is given by a linear system:
  \[
  x_{i+1} = A x_i + B u_i, \quad x_0
  \]
  \[
  y_i = C x_i
  \]
  where:
  \begin{enumerate}
      \item \( x_i \) is the state sequence.
      \item \( u_i \) is the control sequence (which is available to us).
      \item \( y_i \) is the measurement process that we observe.
  \end{enumerate}
  
  The objective is to estimate the states. 
\end{definition}

\begin{claim}
  The optimal closed-loop estimate for a linear system is of the form 
  \[
    \hat{x}_{i+1} = A \hat{x}_i + B u_i + K(y_i - C \hat{x}_i), \quad \hat{x}_0.
  \] 
\end{claim}

\begin{claim}[Dynamics of the Estimation Error]
  The estimation error \( \tilde{x}_{i} \eqdef x_{i} - \hat{x}_{i} \) evolves according to
  \[
  \tilde{x}_{i} = (A - KC)^{i} \tilde{x}_0.
  \]
\end{claim}

\begin{proof}
  We have
  \[
  \tilde{x}_{i+1} = x_{i+1} - \hat{x}_{i+1} = A x_i + B u_i - A \hat{x}_i - B u_i - K(y_i - C \hat{x}_i) = A \tilde{x}_i - K C \tilde{x}_i = (A - KC) \tilde{x}_i.
  \]
  By induction, we conclude the claim.
\end{proof}


\begin{definition}[Observability]
  A discrete-time system is said to be observable if, for any initial state \( x_0 \), the state \( x_k \) can be determined from the output \( y_k \) over a finite time interval \( k = 0, 1, \ldots, N \).
  \end{definition}
  
  \begin{lemma}
  The discrete-time system is observable if and only if the observability matrix \( \mathcal{O} \) defined by:
  \[
  \mathcal{O} = \begin{bmatrix}
  C \\
  CA \\
  CA^2 \\
  \vdots \\
  CA^{n-1}
  \end{bmatrix}
  \]
  has full rank, i.e.,
  \[
  \operatorname{rank}(\mathcal{O}) = n.
  \]
  \end{lemma}






















% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Kalman Filter}


\begin{definition}[Kalman Filter State Space] 
  \begin{equation}
    \begin{aligned}
      x_{i+1} &= \tred{F} x_i + \tred{G} w_i \\
      y_i &= \tred{H} x_i + v_i
    \end{aligned}
  \end{equation}

  where:
\begin{itemize}
    \item \( x_{i+1} \) is the state vector at time \( i+1 \),
    \item \( x_i \) is the state vector at time \( i \),
    \item \( y_i \) is the measurement vector at time \( i \),
    \item \( w_i \) is the process noise (zero mean, uncorrelated),
    \item \( v_i \) is the measurement noise (zero mean, uncorrelated).
\end{itemize}
\end{definition}

\begin{definition}[Estimator]
  An estimator is a sequence of mappings
  \[
  \pi_i = \mathcal{Y}^i \rightarrow \mathcal{X}, \quad i \geq 0
  \]
  such that \( \hat{x}_{i|i} = \pi_i(y^i) \). The corresponding error of an estimator is $\tilde{x}_{i|i} \eqdef x_i - \hat{x}_{i|i}$.
\end{definition}
  
\begin{definition}[Predictor]
  A predictor is a sequence of mappings
  \[
  \pi_i = \mathcal{Y}^{i-1} \rightarrow \mathcal{X}, \quad i \geq 0
  \]
  such that \( \hat{x}_{i|i-1} = \pi_i(y^{i-1}) \). The corresponding error of a predictor is $\tilde{x}_{i|i-1} \eqdef x_i - \hat{x}_{i|i-1}$.
\end{definition}


\begin{definition}[Kalman Filter Covariance Matrix] 
  Formally, the following covariance matrix describes the model:
  \begin{equation}
      \mathbb{E} \left[
      \begin{pmatrix}
          w_i \\
          v_i \\
          x_0
      \end{pmatrix}
      \begin{pmatrix}
          w_j^* & v_j^* & x_0^* & 1
      \end{pmatrix}
      \right] = 
      \begin{pmatrix}
          \begin{pmatrix}
            \tblu{Q} & \tblu{S} \\
            \tblu{S^*} & \tblu{R}
          \end{pmatrix} \delta_{ij} & 0 & 0 \\
          0 & \tblu{\Pi_0} & 0
      \end{pmatrix},
      \label{eq:covariance_matrix}
  \end{equation}
  where \(
    \begin{pmatrix}
      \tblu{Q} & \tblu{S} \\
      \tblu{S^*} & \tblu{R}
    \end{pmatrix}
  \) and \(\tblu{\Pi_0}\) are positive semidefinite matrices and \(\delta_{ij}\) equals 1 if \(i = j\) and is zero otherwise.
  
  Note that \(w_i\) is uncorrelated as a process over time but its coordinates at a fixed time can be correlated via \(Q\).
\end{definition}

\textbf{Markings:}
\begin{itemize}
    \item \tpur{\( P_i \)} - The error covariance matrix at time \( i \)
    \begin{equation}
        \tpur{P_i} \eqdef (x_i - \hat{x}_i)(x_i - \hat{x}_i)^T
    \end{equation}

    \item \tpur{\( R_{e,i} \)} - The covariance of the innovation (or residual) at time \( i \)
    \begin{equation}
        \tpur{R_{e,i}} \eqdef \tred{H} \tpur{P_i} \tred{H^*} + \tblu{R}  
    \end{equation}

    \item \(\tpur{K_{p,i}}\) - The optimal Kalman gain at time \( i \)
    \begin{equation}
        \tpur{K_{p,i}} \triangleq (\tred{F} \tpur{P_i} \tred{H^*} + \tred{G}\tblu{S}) \tpur{R_{e,i}}^{-1}
    \end{equation}

\end{itemize}

\subsection*{Kalman Filter Optimality}

We suggest the following predictor who use the innovation \( y_i - \tred{H} \hat{x}_{i|i-1} \) to update the state estimate:
\begin{equation} \label{eq:predictor}
    \hat{x}_{i+1|i} = \tred{F} \hat{x}_{i|i-1} + \tpur{K_{p,i}} (y_i - \tred{H} \hat{x}_{i|i-1})
\end{equation}

\begin{lemma} 
  \begin{equation}
    \tilde{x}_{i+1} = (F - K_{p,i} H) \tilde{x}_i + (G - K_{p,i}) 
    \begin{pmatrix}
    w_i \\
    v_i
    \end{pmatrix}.
    \end{equation}
\end{lemma}

\begin{proof}
\begin{align*}
\tilde{x}_{i+1} &= x_{i+1} - \hat{x}_{i+1|i} \\
&= (F x_i + G w_i) - \left( F \hat{x}_i + K_{p,i} (y_i - H \hat{x}_i) \right) \\
&= F x_i + G w_i - F \hat{x}_i - K_{p,i} (H x_i + v_i - H \hat{x}_i) \\
&= F x_i + G w_i - F \hat{x}_i - K_{p,i} H x_i - K_{p,i} v_i + K_{p,i} H \hat{x}_i \\
&= F x_i - F \hat{x}_i - K_{p,i} H x_i + K_{p,i} H \hat{x}_i + G w_i - K_{p_i} v_i \\
&= (F - K_{p,i} H)(x_i - \hat{x}_i) + G w_i - K_{p,i} v_i \\
&= (F - K_{p,i} H) \tilde{x}_i + G w_i - K_{p,i} v_i.
\end{align*}
\end{proof}


\begin{theorem}
  For the previous model and predictor, if the error covariance matrix at time \(i\) is $P_i$,
  the matrix \(K_i\) that minimizes \(P_{i+1}\) is given by
  \[
    \tpur{K_{p,i}} \triangleq (\tred{F} \tpur{P_i} \tred{H^*} + \tred{G}\tblu{S}) \tpur{R_{e,i}}^{-1}
  \]
\end{theorem}

The proof uses lemma \ref{eq:predictor} and UDL decomposition.

\begin{theorem}
  The optimal error covariance matrix at time \(i+1\) is given by
  \[
    \tpur{P_{i+1}} = \tred{F}\tpur{P_i}\tred{F^*} + \tred{G}\tblu{Q}\tred{G^*} - \tpur{K_{p,i} R_{e,i} K_{p,i}^*}
  \]
\end{theorem}


\begin{definition}[The Innovations Process]
  The innovations process \( \{e_i\} \) is given by
  \[
  e_i = y_i - \hat{y}_{i|i-1} = y_i - \mathbb{E}[H x_i + v_i | y^{i-1}] = H x_i + v_i - H \hat{x}_i = H (x_i - \hat{x}_i) + v_i = H \tilde{x}_i + v_i.
  \]
\end{definition}


\begin{theorem}
  The innovations process \( \{e_i\} \) is white. That is, we have
  \[
  \mathbb{E}[e_i e_j^T] = R_{e,i} \delta_{ij}.
  \]
\end{theorem}


\begin{lemma}
  For \( j < i \), the recursion can be evolved as
  \begin{align*}
      \tilde{x}_i &= (F - K_{p,i-1}H) \tilde{x}_{i-1} + (G - K_{p,i-1})
      \begin{pmatrix}
          w_{i-1} \\
          v_{i-1}
      \end{pmatrix} \\
      &= \ldots \\
      &= \phi_p(i, j) \tilde{x}_j + \xi_i(j),
  \end{align*}
  where
  \begin{align*}
      \phi_p(i, j) &= \prod_{k=j}^{i-1} (F - K_{p,k}H), \\
      \xi_i(j) &= \sum_{k=j}^{i-1} \phi_p(i, k+1) (G w_k - K_{p,k} v_k).
  \end{align*}
  \end{lemma}  
















% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Stochastic LQR}


\begin{definition}[Stochastic LQR]
  The system is given by
  \[
  x_{t+1} = A x_t + B u_t + w_t,
  \]
  where \(\mathbb{E}[w_t] = 0\), \(\mathbb{E}[w_t w_t^T] = W \delta_{ij}\). The objective function is
  \[
  \min_{\mathbb{E}_{w,x_0}} \left[ x_N^T Q_f x_N + \sum_{t=0}^{N-1} \left( x_t^T Q x_t + u_t^T R u_t \right) \right].
  \]
  As before, we define the value function and derive its recursive formula
  \[
  V_t(z) = \min_{u_t, \ldots, u_{N-1}: x_t = z} \mathbb{E} \left[ x_N^T Q_f x_N + \sum_{\tau=t}^{N-1} \left( x_{\tau}^T Q x_{\tau} + u_{\tau}^T R u_{\tau} \right) \right].
  \]
\end{definition}
  
\begin{theorem}
  For a zero-mean disturbance, the optimal controller in the stochastic setting is equal to the controller in the non-stochastic setting.
\end{theorem}


\begin{lemma}
  The value function is equal to
  \[
  V_t(z) = z^T P_t z + q_t,
  \]
  where \( P_t \) can be computed for \( t = N-1, \ldots, 0 \) as
  \[
  P_t = A^T P_{t+1} A + Q - A^T P_{t+1} B (R + B^T P_{t+1} B)^{-1} B^T P_{t+1} A
  \]
  with the initial condition \( P_N = Q_f \). The additive term can be computed for \( t = N-1, \ldots, 0 \) 
  as \( q_t = q_{t+1} + \text{Tr}(W P_{t+1}) \) with the initial condition \( q_N = 0 \).
\end{lemma}


\begin{lemma}
  If \( \mathbb{E}[w w^T] = W \), then \( \mathbb{E}[w^T P w] = \text{Tr}(W P) \).
\end{lemma}










% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
% * * * * * * * * * * * * * * * * * * * * * * * * 
\newpage
\section{Linear Quadratic Gaussian (LQG) Control}


\begin{definition}[LQG Control]
  The linear-quadratic-Gaussian control concerns linear systems driven by additive white Gaussian noise. We are given a partially observable space state:
  \begin{align}
      x_{t+1} &= A x_t + B u_t + w_t, \\
      y_t &= C x_t + v_t,
  \end{align}
  where the pair \( (w_i, v_i) \sim \mathcal{N}\left(0, \begin{pmatrix}
  W & 0 \\
  0 & V
  \end{pmatrix}\right) \) is an i.i.d. and zero-mean process. The initial state is independent of sequences and is distributed according to \( x_0 \sim \mathcal{N}(0, \Pi_0) \).
  
  The objective is the same as the stochastic LQR problem:
  \[
  J(u) = \mathbb{E} \left[ x_N^T Q_f x_N + \sum_{k=0}^{N-1} \begin{pmatrix}
  x_k \\
  u_k
  \end{pmatrix}^T \begin{pmatrix}
  Q & S^T \\
  S & R
  \end{pmatrix} \begin{pmatrix}
  x_k \\
  u_k
  \end{pmatrix} \right],
  \]
  but the control signals \( u_1, \ldots, u_{N-1} \) are only causal functions of the measurements. That is, the control signal \( u_t \) is a function of \( y_1, \ldots, y_t \).
\end{definition}


\begin{theorem}[Separation Principle]
  The LQG problem can be solved optimally with two separate parts, which facilitate the design:
  \begin{enumerate}
      \item Estimate \( x_t \) based on Kalman Filter: given \((y_1, y_2, \ldots, y_t)\) we construct the estimate \( \hat{x}_{t|t} \). Its error covariance is denoted by \( \Sigma_t \).
      \item Apply LQR control replacing the state with its estimate \( \hat{x}_{t|t} \). That is, the controller is \( u_t = -K_{LQR,t} \hat{x}_{t|t} \).
  \end{enumerate}
\end{theorem}








\end{document}
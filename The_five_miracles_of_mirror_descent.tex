\documentclass[11pt]{book} % or report
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{subcaption}
\usetikzlibrary{positioning}
\usepackage{pgfplots} 
\usepackage[ruled,vlined]{algorithm2e} 
\usepackage{dsfont}

\setlength{\parindent}{0pt}


\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{claim}[section]

\newtheorem*{claim*}{Claim}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\setcounter{tocdepth}{3}


\title{The Five Miracles of Mirror Descent - Sebastian Bubeck}
\author{Hadar Tal}
\date{Winter 2024}

\begin{document}

\frontmatter
\maketitle
\tableofcontents


\mainmatter
\chapter{Mathematical Background}

\section{Multivariable Calculus}
\begin{definition}{Diffrentiability, single variable} \\
Let $f: (a,b) \rightarrow \mathbb{R}$ be a function. We say that $f$ is differentiable at $x_0 \in (a,b)$ if
\begin{equation}
    \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h}
\end{equation}
exists. If $f$ is differentiable at $x_0$, then $f'(x_0)$ is the derivative of $f$ at $x_0$.
\end{definition} 

\bigbreak

\begin{definition}{Diffrentiability, single variable (alternative)} \\
Let $f: (a,b) \rightarrow \mathbb{R}$ be a function. We say that $f$ is differentiable at $x_0 \in (a,b)$ if there exists a number m such that:
\begin{equation}
    f(x_0 + h) = f(x_0) + m \cdot h + E(h) \text{ where } \lim_{h \rightarrow 0} \frac{E(h)}{h} = 0
\end{equation}
If $f$ is differentiable at $x_0$, then $f'(x_0) = m$ is the derivative of $f$ at $x_0$.
\end{definition}

\bigbreak

\begin{definition}{Diffrentiability, multivariable} \\
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function. We say that $f$ is differentiable at $x_0$ if there exists a vector m $\in \mathbb{R}^n$ such that:
\begin{equation}
    \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0) - m \cdot h}{||h||} = 0
\end{equation}
If $f$ is differentiable at $x_0$, then $m$ is the gradient of $f$ at $x_0$, denoted $\nabla f(x_0)$.
\end{definition}

\bigbreak

Suppose the $S \subseteq \mathbb{R}^n$ and $f: S \rightarrow \mathbb{R}$ is a function. 

\bigbreak

\begin{definition}{Limit, multivariate function} \\
We say that the limit of $f$ at $x_0$ is $L$ if for all $\epsilon > 0$, there exists $\delta > 0$ such that 
for all $x$ such that $||x - x_0|| < \delta$, we have $|f(x) - L| < \epsilon$.
\end{definition}

\begin{definition}{Diffrentiability, multivariable (alternative)} \\
We say that $f$ is differentiable at $x_0$ if there exists a vector m $\in \mathbb{R}^n$ such that:
\begin{equation}
    f(x_0 + h) = f(x_0) + m^T \cdot h + E(h) \text{ where } \lim_{h \rightarrow 0} \frac{E(h)}{||h||} = 0
\end{equation}
If $f$ is differentiable at $x_0$, then $m$ is the gradient of $f$ at $x_0$, denoted $\nabla f(x_0)$.
\end{definition}

\bigbreak

\begin{definition}{Partial Derivative} \\
The partial derivative of $f$ with respect to the $i$-th variable at $x$ is:
\begin{equation}
    \frac{\partial f}{\partial x_i}(x) = \lim_{h \rightarrow 0} \frac{f(x + h \cdot e_i) - f(x)}{h}
\end{equation}
where $e_i$ is the $i$-th standard basis vector.
\end{definition}

\bigbreak

\begin{theorem}(Diffrentiability vs. Partial Derivatives) \\
If $f$ is differentiable at $x$, then all partial derivatives of $f$ exist at $x$ and:
\begin{equation}
    \nabla f(x) = \left( \frac{\partial f}{\partial x_1}(x), \ldots, \frac{\partial f}{\partial x_n}(x) \right)
\end{equation}    
\end{theorem}

\bigbreak

\begin{itemize}
    \item If any partial derivative of $f$ does not exist at $x$, then $f$ is not differentiable at $x$.
    \item If all partial derivatives of $f$ exist at $x$, then $f$ may still not be differentiable at $x$ 
    and the vector $m = \nabla f(x)$ is the only possible vector that satisfies the definition of differentiability.
\end{itemize}

\bigbreak

\begin{definition}{Continuously Differentiable} \\
We say that $f$ is continuously differentiable or of class $C^1$  if all partial derivatives of $f$ exist and are continuous at every point in $S$.
\end{definition}

\bigbreak

\begin{theorem}
If $f$ is continuously differentiable, then $f$ is differentiable.
\end{theorem}

\bigbreak

\begin{definition}{The directional derivative} \\
For a given $x \in S$ and a unit vector $u \in \mathbb{R}^n$, the directional derivative of $f$ at $x$ in the direction of $u$ is:
\begin{equation}
    \partial_u f(x) = \lim_{h \rightarrow 0} \frac{f(x + h \cdot u) - f(x)}{h}
\end{equation}
Equivalently, $\partial_u f(x) = g'(0)$ where $g(h) = f(x + h \cdot u)$.
\end{definition}

\bigbreak

\begin{theorem}
If $f$ is differentiable at $x$, then for all $u \in \mathbb{R}^n$, the directional derivative of $f$ at $x$ in the direction of $u$ exists and is given by:
\begin{equation}
    \partial_u f(x) = \nabla f(x) \cdot u
\end{equation}
\end{theorem}

\bigbreak

\begin{theorem}{Fermat's Theorem} \\
If $f$ is differentiable at $x$ and $x$ is a local minimum of $f$, then $\nabla f(x) = 0$.
\end{theorem}

\bigbreak

\begin{theorem}
Suppose that $f: S \rightarrow \mathbb{R}$ is differentiable at $x$. Then $\nabla f(x)$ is orthogonal to the level set of $f$ that passes through $x$.    
\end{theorem}

\bigbreak

\begin{theorem}{The mean value theorem} \\
If $f: S \rightarrow \mathbb{R}$ is differentiable on the open interval between $a$ and $b$, then there exists $c \in [a,b]$ such that:
\begin{equation}
    f(b) - f(a) = \nabla f(c) \cdot (b-a)
\end{equation}
where $[a,b] = {a + t(b-a) | t \in [0,1]}$.
\end{theorem}

\bigbreak

\begin{definition}{Second-order partial derivatives} \\
Suppose that f is a $C^1$ function. If the partial derivatives of f are differentiable, then the second-order partial derivatives of f are:
\begin{equation}
    \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial}{\partial x_i} \left( \frac{\partial f}{\partial x_j} \right) 
\end{equation}
Equivalently, $\frac{\partial^2 f}{\partial i \partial j} = \partial_j \partial_j f$.
If i = j we denote $\frac{\partial^2 f}{\partial x_i^2}$ or $(\partial_i^2 f$
\end{definition}

\bigbreak

\begin{definition}{The $C^2$ class} \\
We say that $f$ is of class $C^2$ if all second-order partial derivatives of $f$ exist and are continuous.
\end{definition}

\bigbreak

\begin{theorem}{Clairaut's Theorem} \\
If $f$ is of class $C^2$, then $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$.
\end{theorem}

\bigbreak

\begin{definition}{Hessian Matrix} \\
The Hessian matrix of $f$ at $x$ is the matrix of second-order partial derivatives of $f$ at $x$:
\begin{equation}
    \nabla^2 f(x) = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \ldots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_n^2}
    \end{bmatrix}
\end{equation}
\end{definition}

\bigbreak

\begin{corollary*}{The interpretation of the Hessian matrix} \\
Let $u \in \mathbb{R}^n$ be a unit vector. then
\begin{equation}
    \partial_{uu}^2 f(x) = \sum_{i,j=1}^n \partial_{ij} f(x) u_i u_j  = u^T \nabla^2 f(x) u
\end{equation}
\end{corollary*}

\bigbreak

\section{Taylor series}
\begin{definition}{Taylor Series} \\
Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a function that is $k$ times differentiable at $x_0$. Then the Taylor series of $f$ at $x_0$ is given by:
\begin{equation}
    f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 + \ldots + \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + R_k(x)
\end{equation}
where $R_k(x) = \frac{f^{(k+1)}(c)}{(k+1)!}(x-x_0)^{k+1}$ for some $c$ between $x$ and $x_0$.
\end{definition}

\bigbreak

\begin{definition}{Taylor Series for Multivariable Functions (k=2) } \\
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function that is $C^2$ at $x_0$. Then for any h such that $x_0 + h \in S$, there exists $\theta \in [0,1]$ such that:
\begin{equation}
    f(x_0 + h) = f(x_0) + \nabla f(x_0) \cdot h + \frac{1}{2} h^T \nabla^2 f(x_0 + \theta h) h
\end{equation}
\end{definition}

\bigbreak

\section{Important Inequalities}

\subsection{1 + x $\leq$ e$^{x}$}

% \begin{proof}

    % \begin{definition}{Subgradient} \\
    %     Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a convex function. A vector $g \in \mathbb{R}^n$ is a subgradient of $f$ at $x$ if for all $y \in \mathbb{R}^n$:
    %     \begin{equation}
    %         f(y) \geq f(x) + g^T(y-x)
    %     \end{equation}
    %     \end{definition}



\end{document}
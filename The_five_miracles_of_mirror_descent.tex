\documentclass[11pt]{book} % or report
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{subcaption}
\usetikzlibrary{positioning}
\usepackage{pgfplots} 
\usepackage[ruled,vlined]{algorithm2e} 
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{mathdesign}
\usepackage{float}


\setlength{\parindent}{0pt}


\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{claim}[section]

\newtheorem*{claim*}{Claim}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\setcounter{tocdepth}{3}




\begin{document}

\begin{titlepage}
    \begin{center}
     {\huge\bfseries The Five Miracles of Mirror Descent \\}
     % ----------------------------------------------------------------
     \vspace{1.5cm}
     {\Large\bfseries Hadar Tal}\\[5pt]
     hadar.tal@mail.huji.ac.il\\[14pt]
      % ----------------------------------------------------------------
     \vspace{2cm}
     {This paper is a summary of the educational materials and lectures from Professor Sebastian Bubeck, enhanced by Claire Boyer's comprehensive notes, and structured according to Tomer Koren's course on Optimization for Computer Science.} \\[2cm]

     \vfill
    {Winter 2024}
    \end{center}
\end{titlepage}


\frontmatter
\tableofcontents

\mainmatter
\chapter{Mathematical Background}

\section{Multivariable Calculus}
\begin{definition}{Diffrentiability, single variable} \\
Let $f: (a,b) \rightarrow \mathbb{R}$ be a function. We say that $f$ is differentiable at $x_0 \in (a,b)$ if
\begin{equation}
    \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0)}{h}
\end{equation}
exists. If $f$ is differentiable at $x_0$, then $f'(x_0)$ is the derivative of $f$ at $x_0$.
\end{definition} 

\bigbreak

\begin{definition}{Diffrentiability, single variable (alternative)} \\
Let $f: (a,b) \rightarrow \mathbb{R}$ be a function. We say that $f$ is differentiable at $x_0 \in (a,b)$ if there exists a number m such that:
\begin{equation}
    f(x_0 + h) = f(x_0) + m \cdot h + E(h) \text{ where } \lim_{h \rightarrow 0} \frac{E(h)}{h} = 0
\end{equation}
If $f$ is differentiable at $x_0$, then $f'(x_0) = m$ is the derivative of $f$ at $x_0$.
\end{definition}

\bigbreak

\begin{definition}{Diffrentiability, multivariable} \\
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function. We say that $f$ is differentiable at $x_0$ if there exists a vector m $\in \mathbb{R}^n$ such that:
\begin{equation}
    \lim_{h \rightarrow 0} \frac{f(x_0 + h) - f(x_0) - m \cdot h}{||h||} = 0
\end{equation}
If $f$ is differentiable at $x_0$, then $m$ is the gradient of $f$ at $x_0$, denoted $\nabla f(x_0)$.
\end{definition}

\bigbreak

Suppose the $S \subseteq \mathbb{R}^n$ and $f: S \rightarrow \mathbb{R}$ is a function. 

\bigbreak

\begin{definition}{Limit, multivariate function} \\
We say that the limit of $f$ at $x_0$ is $L$ if for all $\epsilon > 0$, there exists $\delta > 0$ such that 
for all $x$ such that $||x - x_0|| < \delta$, we have $|f(x) - L| < \epsilon$.
\end{definition}

\begin{definition}{Diffrentiability, multivariable (alternative)} \\
We say that $f$ is differentiable at $x_0$ if there exists a vector m $\in \mathbb{R}^n$ such that:
\begin{equation}
    f(x_0 + h) = f(x_0) + m^T \cdot h + E(h) \text{ where } \lim_{h \rightarrow 0} \frac{E(h)}{||h||} = 0
\end{equation}
If $f$ is differentiable at $x_0$, then $m$ is the gradient of $f$ at $x_0$, denoted $\nabla f(x_0)$.
\end{definition}

\bigbreak

\begin{definition}{Partial Derivative} \\
The partial derivative of $f$ with respect to the $i$-th variable at $x$ is:
\begin{equation}
    \frac{\partial f}{\partial x_i}(x) = \lim_{h \rightarrow 0} \frac{f(x + h \cdot e_i) - f(x)}{h}
\end{equation}
where $e_i$ is the $i$-th standard basis vector.
\end{definition}

\bigbreak

\begin{theorem}(Diffrentiability vs. Partial Derivatives) \\
If $f$ is differentiable at $x$, then all partial derivatives of $f$ exist at $x$ and:
\begin{equation}
    \nabla f(x) = \left( \frac{\partial f}{\partial x_1}(x), \ldots, \frac{\partial f}{\partial x_n}(x) \right)
\end{equation}    
\end{theorem}

\bigbreak

\begin{itemize}
    \item If any partial derivative of $f$ does not exist at $x$, then $f$ is not differentiable at $x$.
    \item If all partial derivatives of $f$ exist at $x$, then $f$ may still not be differentiable at $x$ 
    and the vector $m = \nabla f(x)$ is the only possible vector that satisfies the definition of differentiability.
\end{itemize}

\bigbreak

\begin{definition}{Continuously Differentiable} \\
We say that $f$ is continuously differentiable or of class $C^1$  if all partial derivatives of $f$ exist and are continuous at every point in $S$.
\end{definition}

\bigbreak

\begin{theorem}
If $f$ is continuously differentiable, then $f$ is differentiable.
\end{theorem}

\bigbreak

\begin{definition}{The directional derivative} \\
For a given $x \in S$ and a unit vector $u \in \mathbb{R}^n$, the directional derivative of $f$ at $x$ in the direction of $u$ is:
\begin{equation}
    \partial_u f(x) = \lim_{h \rightarrow 0} \frac{f(x + h \cdot u) - f(x)}{h}
\end{equation}
Equivalently, $\partial_u f(x) = g'(0)$ where $g(h) = f(x + h \cdot u)$.
\end{definition}

\bigbreak

\begin{theorem}
If $f$ is differentiable at $x$, then for all $u \in \mathbb{R}^n$, the directional derivative of $f$ at $x$ in the direction of $u$ exists and is given by:
\begin{equation}
    \partial_u f(x) = \nabla f(x) \cdot u
\end{equation}
\end{theorem}

\bigbreak

\begin{theorem}{Fermat's Theorem} \\
If $f$ is differentiable at $x$ and $x$ is a local minimum of $f$, then $\nabla f(x) = 0$.
\end{theorem}

\bigbreak

\begin{theorem}
Suppose that $f: S \rightarrow \mathbb{R}$ is differentiable at $x$. Then $\nabla f(x)$ is orthogonal to the level set of $f$ that passes through $x$.    
\end{theorem}

\bigbreak

\begin{theorem}{The mean value theorem} \\
If $f: S \rightarrow \mathbb{R}$ is differentiable on the open interval between $a$ and $b$, then there exists $c \in [a,b]$ such that:
\begin{equation}
    f(b) - f(a) = \nabla f(c) \cdot (b-a)
\end{equation}
where $[a,b] = {a + t(b-a) | t \in [0,1]}$.
\end{theorem}

\bigbreak

\begin{definition}{Second-order partial derivatives} \\
Suppose that f is a $C^1$ function. If the partial derivatives of f are differentiable, then the second-order partial derivatives of f are:
\begin{equation}
    \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial}{\partial x_i} \left( \frac{\partial f}{\partial x_j} \right) 
\end{equation}
Equivalently, $\frac{\partial^2 f}{\partial i \partial j} = \partial_j \partial_j f$.
If i = j we denote $\frac{\partial^2 f}{\partial x_i^2}$ or $(\partial_i^2 f$
\end{definition}

\bigbreak

\begin{definition}{The $C^2$ class} \\
We say that $f$ is of class $C^2$ if all second-order partial derivatives of $f$ exist and are continuous.
\end{definition}

\bigbreak

\begin{theorem}{Clairaut's Theorem} \\
If $f$ is of class $C^2$, then $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$.
\end{theorem}

\bigbreak

\begin{definition}{Hessian Matrix} \\
The Hessian matrix of $f$ at $x$ is the matrix of second-order partial derivatives of $f$ at $x$:
\begin{equation}
    \nabla^2 f(x) = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \ldots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2 f}{\partial x_n^2}
    \end{bmatrix}
\end{equation}
\end{definition}

\bigbreak

\begin{corollary*}{The interpretation of the Hessian matrix} \\
Let $u \in \mathbb{R}^n$ be a unit vector. then
\begin{equation}
    \partial_{uu}^2 f(x) = \sum_{i,j=1}^n \partial_{ij} f(x) u_i u_j  = u^T \nabla^2 f(x) u
\end{equation}
\end{corollary*}

\bigbreak

\section{Taylor series}
\begin{definition}{Taylor Series} \\
Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a function that is $k$ times differentiable at $x_0$. Then the Taylor series of $f$ at $x_0$ is given by:
\begin{equation}
    f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 + \ldots + \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + R_k(x)
\end{equation}
where $R_k(x) = \frac{f^{(k+1)}(c)}{(k+1)!}(x-x_0)^{k+1}$ for some $c$ between $x$ and $x_0$.
\end{definition}

\bigbreak

\begin{definition}{Taylor Series for Multivariable Functions (k=2) } \\
Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function that is $C^2$ at $x_0$. Then for any h such that $x_0 + h \in S$, there exists $\theta \in [0,1]$ such that:
\begin{equation}
    f(x_0 + h) = f(x_0) + \nabla f(x_0) \cdot h + \frac{1}{2} h^T \nabla^2 f(x_0 + \theta h) h
\end{equation}
\end{definition}

\bigbreak

% *
% *
% *
% *
% *

\section{Important subsets of $\mathbb{R}^n$}

\begin{definition}{Open set} \\
A set $S \subseteq \mathbb{R}^n$ is open if for all $x \in S$, there exists $\epsilon > 0$ such that $B(x, \epsilon) \subseteq S$.
\end{definition}

\begin{definition}{Closed set} \\
A set $S \subseteq \mathbb{R}^n$ is closed if its complement is open.
\end{definition}

\begin{definition}{Interior point} \\
A point $x \in S$ is an interior point of $S$ if there exists $\epsilon > 0$ such that $B(x, \epsilon) \subseteq S$.
\end{definition}

\begin{corollary}{Open set characterization} \\
A set $S \subseteq \mathbb{R}^n$ is open if and only if every point in $S$ is an interior point of $S$.
\end{corollary}

\begin{definition}{Boundary point} \\
A point $x \in S$ is a boundary point of $S$ if for all $\epsilon > 0$, $B(x, \epsilon) \cap S \neq \emptyset$ and $B(x, \epsilon) \cap S^c \neq \emptyset$.
\end{definition}

\begin{definition}{Half-space} \\
A half-space in $\mathbb{R}^n$ is a set of the form $\{x \in \mathbb{R}^n : a^T x \leq b\}$ for some $a \in \mathbb{R}^n$ and $b \in \mathbb{R}$.
\end{definition}

\begin{definition}{Hyperplane} \\
A hyperplane in $\mathbb{R}^n$ is a set of the form $\{x \in \mathbb{R}^n : a^T x = b\}$ for some $a \in \mathbb{R}^n$ and $b \in \mathbb{R}$.
\end{definition}

\begin{definition}{Polyhedron (Polyhedra)} \\
A polyhedron in $\mathbb{R}^n$ is a set of the form $\{x \in \mathbb{R}^n : Ax \leq b\}$ for some $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$.
Equivalently, a polyhedron is the intersection of finitely many half-spaces.
\end{definition}

\begin{definition}{Polytope} \\
A polytope in $\mathbb{R}^n$ is a bounded polyhedron - i.e., there exists $r > 0$ such that $\forall x \in \{x \in \mathbb{R}^n : Ax \leq b\} \implies ||x|| \leq r$.
Equivalently, a polytope is the convex hull of finitely many points.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Figs/polyhedron.png}
    \caption{Polytope}
\end{figure}

\begin{definition}{Convex set} \\
    A set $S \subseteq \mathbb{R}^n$ is convex if for all $x,y \in S$ and $\lambda \in [0,1]$, we have $\lambda t + (1- \lambda)y \in S$.
\end{definition}    

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/convex_set.png}
    \caption{Convex set}
\end{figure}

\begin{definition}{Convex hull} \\
The convex hull of a set $S \subseteq \mathbb{R}^n$ is the smallest convex set that contains $S$.
\end{definition}


\begin{definition}{Conic combination} \\
A point $x \in \mathbb{R}^n$ is a conic combination of $y_1, \ldots, y_k \in \mathbb{R}^n$ 
if there exist $\lambda_1, \ldots, \lambda_k \geq 0$ such that $x = \sum_{i=1}^k \lambda_i y_i$.    
\end{definition}

\begin{definition}{Conic hull} \\
The conic hull of a finite set $S \subseteq \mathbb{R}^n$ is the set of all conic combinations of points in $S$.
\end{definition}


\begin{definition}{Convex cone} \\
A set $S \subseteq \mathbb{R}^n$ is a convex cone if for all $x \in S$ and $\lambda \geq 0$, we have $\lambda x \in S$.
\end{definition}

\begin{figure}[H]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figs/circular-pyramid.png}
        \caption{Convex cone that is not a conic hull of finitely many generators.}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figs/polyhedral_cone.png}
        \caption{Convex cone genrated by the conic combination of three black vectors (conic hull).}
    \end{subfigure}
\end{figure}

\begin{definition}{Normal cone} \\
The normal cone to a set $S$ at a point $x$ is defined as
\begin{equation}
    N_S(x) = \{ v \in \mathbb{R}^n : \langle v, y-x \rangle \leq 0 \text{ for all } y \in S \}
\end{equation}
\end{definition}

\begin{definition}{Tangent cone} \\
The tangent cone to a set $S$ at a point $x$ is defined as
\begin{equation}
    T_S(x) = \{ v \in \mathbb{R}^n : \lim_{t \rightarrow 0^+} \frac{x + tv - x}{t} \in S \}
\end{equation}
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/normal-cones.png}
    \caption{Normal and tangent cones}
\end{figure}

\begin{theorem}{Normal cone of polyhedron} \\
The normal cone to a polyhedron $S = \{x \in \mathbb{R}^n : \forall j \in [m] \quad a_j \cdot x \leq b_j\}$ at a point $x$ is given by
\begin{equation}
    N_S(x) = \{ \sum_{j} \lambda_j a_j : \lambda_j \geq 0  \text{ and }  a_j \cdot x = b_j \}
\end{equation}
    
\end{theorem}

\section{Convexity}

\subsection{Definitions and Fundamental Theorems}



\begin{definition} (Convex function): A function $f : S \rightarrow \mathbb{R}$ defined on a convex set $S$ is convex if, for all $x, y \in S$ and $\lambda \in [0,1]$, 
\[ f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda) f(y). \]
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/convex_function.png}
    \caption{Convex function}
\end{figure}

\begin{theorem} (Characterization via epigraph): A function $f : S \to \mathbb{R}$ is convex if and only if its epigraph $\{(x,t) \in S \times \mathbb{R} : f(x) \leq t\}$ is a convex set.
\end{theorem}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Figs/epigraph_of_convex_function.png}
    \caption{Epigraph of a convex function}
\end{figure}


\begin{claim} (Convexity of sublevel sets): If $f : S \to \mathbb{R}$ is convex, then the sublevel set $S_t = \{ x \in S : f(x) \leq t\}$ is convex for any $t \in \mathbb{R}$.
\end{claim}

\subsection{Inequalities and Characterizations}

\begin{theorem}(Jensen's inequality): If $f$ is a convex function, then for any $x_1, x_2, \ldots, x_n \in S$ and any non-negative weights $\alpha_i$ such that $\sum_{i=1}^n \alpha_i = 1$,
\[ f\left(\sum_{i=1}^n \alpha_i x_i\right) \leq \sum_{i=1}^n \alpha_i f(x_i). \]
\end{theorem}


\begin{theorem}(First-order characterization, aka “the gradient inequality”): If $f$ is a differentiable convex function on an open set $S$, then for all $x, y \in S$,
\[ f(y) \geq f(x) + \nabla f(x)^\top (y - x). \]
\end{theorem}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Figs/first_order_characterization_of_convexity.png}
    \caption{First-order characterization of convexity}
\end{figure}


\begin{theorem} (Jensen’s inequality, generalized for expectation): If $f$ is a convex function and $X$ is a random variable over $S$, then
\[ f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]. \]
\end{theorem}


\begin{theorem} (Second-order characterization of convexity): A twice differentiable function $f$ is convex on an open set $S$ if and only if the Hessian matrix of $f$ is positive semidefinite at every point in $S$.
\end{theorem}

\subsection{Optimization and Projection}

\begin{definition} (Convex optimization): The problem of minimizing a convex function over a convex set.
\end{definition}

\begin{theorem} (Optimality conditions, unconstrained): If $f$ is convex and differentiable, \\
     $x^*$ is a local minimum of $f$ $\Leftrightarrow$ $x^*$ is a global minimum of $f$ $\Leftrightarrow$  $\nabla f(x^*) = 0$.
\end{theorem}

\begin{theorem} (Optimality conditions, constrained): If $f$ is differentiable and $C$ is a convex set,
     $x^*$ is a local minimum of $f$ on $C$ if and only if $\langle \nabla f(x^*), x - x^* \rangle \geq 0$ for all $x \in C$.
\end{theorem}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/optimality_condition_constrained.png}
    \caption{Optimality conditions, constrained}
\end{figure}


\begin{corollary}{Optimality conditions, constrained (alternative)} \\
If $f$ is differentiable and $C$ is a convex set, then $x^*$ is a local minimum of $f$ on $C$ if and only if $-\nabla f(x^*) \in N_C(x^*)$.
\end{corollary}

\begin{definition} (Projection): The projection of a point $x$ onto a convex set $S$ is defined as $\Pi_S(x) = \arg\min_{y \in S} \|y-x\|$.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Figs/projection.png}
    \caption{Projection}
\end{figure}


\begin{theorem} {Generalized cosine theorem} \\
    Let \( S \subseteq \mathbb{R}^d \) be convex and \( x \in \mathbb{R}^d \). Then the projection \( \Pi_S[x] \) is unique and satisfies:
    \begin{equation}
    \| x - \Pi_S[x] \|^2 + \| \Pi_S[x] - y \|^2 \leq \| x - y \|^2, \quad \forall y \in S.
    \end{equation}
    In particular:
    \begin{equation}
    \| \Pi_S[x] - y \| \leq \| x - y \|, \quad \forall y \in S.
    \end{equation}
\end{theorem}

% *
% *
% *
% *
% *

\section{Properties of Convex Functions}

% *
% *
% *
% *
% *

\section{Important Inequalities}

\subsection{1 + x $\leq$ e$^{x}$}




\end{document}
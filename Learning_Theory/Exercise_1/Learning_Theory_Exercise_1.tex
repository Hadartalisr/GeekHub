\documentclass[a4 paper]{article}
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,tikz,amssymb}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing} 
\usetikzlibrary{positioning}
\usepackage{float}
\usepackage{minted}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
% \usepackage[hidelinks]{hyperref}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{mathdesign}
\usepackage{float}
\usepackage{todonotes} 
\usepackage{empheq}
\usepackage{array}
\usepackage[ruled,vlined]{algorithm2e} 
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\usepackage{bookmark}

\usepackage{pgfplots} 
\pgfplotsset{compat=1.18}


\newtcolorbox{boxA}{
    fontupper = \bf,
    boxrule = 1.5pt,
    colframe = black % frame color
}


\setlength{\parindent}{0pt}
% \numberwithin{equation}


\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\newcommand\defeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\SetCommentSty{mycommfont}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}



\newtheoremstyle{boldStyle}%                % Name
  {}%                                     % Space above
  {}%                                     % Space below
  {\itshape}%                                     % Body font
  {}%                                     % Indent amount
  {\bfseries}%                            % Theorem head font
  {}%                                    % Punctuation after theorem head
  {\newline}                              % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%                                     % Theorem head spec (can be left empty, meaning `normal')


\theoremstyle{boldStyle}
\newtheorem{example}{Example}[section]
\newtheorem{question}{Question}[section]
\newtheorem{answer}{Answer}[section]


\newtheorem*{claim*}{Claim}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{remark*}{Remark}
\newtheorem*{example*}{Example}
\newtheorem*{examples*}{Examples}
\newtheorem*{definition*}{Definition}
\newtheorem*{question*}{Question}
\newtheorem*{answer*}{Answer}


\definecolor{blueColor}{rgb}{0, 0.611, 0.98} 
\newtheoremstyle{boldBlueStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{blueColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldBlueStyle}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{claim}[section]
\newtheorem{proposition}{Proposition}[section]



\definecolor{purpleColor}{rgb}{0.59, 0.223, 0.6} 
\newtheoremstyle{boldPurpleStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{purpleColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldPurpleStyle}
\newtheorem{theorem}{Theorem}[section]


\definecolor{redColor}{rgb}{1, 0.219, 0.219} 
\newtheoremstyle{boldRedStyle}%                % Name
  {}%                                          % Space above
  {}%                                          % Space below
  {\itshape}%                                  % Body font
  {}%                                          % Indent amount
  {\color{redColor}\bfseries}%          % Theorem head font in red
  {}%                                          % Punctuation after theorem head
  {\newline}%                                  % Space after theorem head, new line
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{boldRedStyle}
\newtheorem{definition}{Definition}[section]




%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={Ashudeep Singh},%
pdftitle={Homework},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
% \usepackage[francais]{babel}
\usepackage{booktabs}
\input{../../Latex_Utils/macros.tex}

\setlength{\parindent}{0pt}


\begin{document}
\homework{67939 - Topics in Learning Theory}{Exercise 1}{Due: 16/06/24}{Prof. Amit Daniely}{}{Hadar Tal}{}



\section*{Exercise 1}
\textcolor{blueColor}{
The moment generating function (MGF) of a random variable \(X\) is \(M_X(\lambda) = \mathbb{E}[e^{\lambda X}]\). 
Assume that \(M_X\) is defined for any \(\lambda\) in a non-empty segment \((-a, a)\). Show that
}

\begin{enumerate}

\item \textcolor{blueColor}{ \(M_X^{(k)}(0) = \mathbb{E}[X^k]\) }

Using the definition of the moment-generating function, we can write:
\[
M_X^{(k)}(t) = \frac{d^k}{d\lambda^k} \mathbb{E}[e^{\lambda X}] 
\]

Using the power series expansion of the exponential function
\[
e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}
\]

we can write
\[
M_X^{(k)}(t) = \frac{d^k}{d\lambda^k} \mathbb{E} \left( \sum_{m=0}^{\infty} \frac{\lambda^m X^m}{m!} \right)
\]

Because the expected value is a linear operator, we have:
\[
M_X^{(k)}(t) = \frac{d^k}{d\lambda^k} \sum_{m=0}^{\infty} \mathbb{E} \left( \frac{\lambda^m X^m}{m!} \right) 
  = \sum_{m=0}^{\infty} \frac{d^k}{d\lambda^k} \left( \frac{\lambda^m}{m!} \right) \mathbb{E}[X^m]
\]

Using the \(k\)-th derivative of the \(m\)-th power
\[
\frac{d^k}{d\lambda^k} \lambda^m = 
\begin{cases} 
\tilde{m}^k \lambda^{m-k}, & \text{if } k \leq m \\
0, & \text{if } k > m
\end{cases} 
\]

when 
\[
\tilde{m}^k  = \prod_{i=0}^{k-1} (m-i) = \frac{m!}{(m-k)!}
\]

then we have
\begin{align*}
M_X^{(k)}(\lambda) &= \sum_{m=0}^{\infty} \frac{d^k}{d\lambda^k} \left( \frac{\lambda^m}{m!} \right) \mathbb{E}[X^m] = \sum_{m=k}^{\infty} \frac{\tilde{m^k} \lambda^{m-k}}{m!} \mathbb{E}[X^m] 
  = \sum_{m=k}^{\infty} \frac{m! \lambda^{m-k}}{(m-k)! m!} \mathbb{E}[X^m] \\
  &= \sum_{m=k}^{\infty} \frac{\lambda^{m-k}}{(m-k)!} \mathbb{E}[X^m] = \frac{t^{n-n}}{(n-n)!} \mathbb{E}[X^n] + \sum_{m=k+1}^{\infty} \frac{\lambda^{m-k}}{(m-k)!} \mathbb{E}[X^m] \\
  &= \mathbb{E}[X^k] + \sum_{m=k+1}^{\infty} \frac{\lambda^{m-k}}{(m-k)!} \mathbb{E}[X^m]
\end{align*}

Setting \(\lambda = 0\) in the above equation, we get
\[
M_X^{(k)}(0) = \mathbb{E}[X^k] + \sum_{m=k+1}^{\infty} \frac{0^{m-k}}{(m-k)!} \mathbb{E}[X^m] = \mathbb{E}[X^k]
\]
which completes the proof.


    
\newpage
\item \textcolor{blueColor}{Show that for a centered Gaussian \(X\) with variance \(\sigma^2\), \(M_X(\lambda) = e^{\frac{\lambda^2 \sigma^2}{2}}\). 
In other words, being \(\sigma\)-SubGaussian is equivalent to having MGF that is bounded by the MGF of a centered Gaussian with variance \(\sigma^2\).}
    



Let \(X\) be a centered Gaussian random variable with mean \(\mathbb{E}[X] = 0\) and variance \(\mathrm{var}(X) = \sigma^2\). The moment generating function (MGF) of \(X\) is defined as:
\[
M_X(\lambda) = \mathbb{E}[e^{\lambda X}].
\]

Since \(X\) is Gaussian, \(X\) has the probability density function:
\[
f_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2\sigma^2}}
\]

Therefore, the MGF \(M_X(\lambda)\) is:
\[
M_X(\lambda) = \int_{-\infty}^{\infty} e^{\lambda x} f_X(x) \, dx = \int_{-\infty}^{\infty} e^{\lambda x} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2\sigma^2}} \, dx
\]

Combining the exponents, we get:
\[
M_X(\lambda) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{\lambda x - \frac{x^2}{2\sigma^2}} \, dx
\]

Completing the square in the exponent:
\[
\lambda x - \frac{x^2}{2\sigma^2} = -\frac{1}{2\sigma^2} \left(x^2 - 2\sigma^2 \lambda x \right) = -\frac{1}{2\sigma^2} \left(x^2 - 2\sigma^2 \lambda x + \sigma^4 \lambda^2 - \sigma^4 \lambda^2 \right) = -\frac{1}{2\sigma^2} \left((x - \sigma^2 \lambda)^2 - \sigma^4 \lambda^2 \right).
\]

Thus, the integral becomes:
\[
M_X(\lambda) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \sigma^2 \lambda)^2} e^{\frac{\sigma^2 \lambda^2}{2}} \, dx
\]

Since the first term inside the integral is a normal distribution that integrates to 1, we get:
\[
M_X(\lambda) = e^{\frac{\sigma^2 \lambda^2}{2}} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \sigma^2 \lambda)^2} \, dx = e^{\frac{\sigma^2 \lambda^2}{2}}
\]

Therefore, the MGF of \(X\) is:
\[
M_X(\lambda) = e^{\frac{\lambda^2 \sigma^2}{2}}
\]

This shows that being \(\sigma\)-SubGaussian is equivalent to having an MGF that is bounded by the MGF of a centered Gaussian with variance \(\sigma^2\).









    
    
\newpage
\item \textcolor{blueColor}{Show that if \(X\) is uniform over \([a, b]\) then \(M_X(\lambda) = \frac{e^{\lambda b} - e^{\lambda a}}{\lambda (b - a)}\).}

Let \(X\) be a random variable uniformly distributed over the interval \([a, b]\). The probability density function of \(X\) is:
\[
f_X(x) = \frac{1}{b-a}, \quad \text{for } a \leq x \leq b
\]

The moment generating function (MGF) of \(X\) is defined as:
\[
M_X(\lambda) = \mathbb{E}[e^{\lambda X}] = \int_{a}^{b} e^{\lambda x} f_X(x) \, dx
\]

Substituting the PDF of \(X\):
\[
M_X(\lambda) = \int_{a}^{b} e^{\lambda x} \frac{1}{b-a} \, dx
\]

Since \(\frac{1}{b-a}\) is a constant, we can factor it out:
\[
M_X(\lambda) = \frac{1}{b-a} \int_{a}^{b} e^{\lambda x} \, dx
\]

To solve the integral, we use the antiderivative of \(e^{\lambda x}\):
\[
\int e^{\lambda x} \, dx = \frac{1}{\lambda} e^{\lambda x} + C
\]

Evaluating this from \(a\) to \(b\), we get:
\[
\int_{a}^{b} e^{\lambda x} \, dx = \left. \frac{1}{\lambda} e^{\lambda x} + C \right|_{a}^{b} = \frac{1}{\lambda} \left( e^{\lambda b} - e^{\lambda a} \right).
\]

Therefore,
\[
M_X(\lambda) = \frac{1}{b-a} \cdot \frac{1}{\lambda} \left( e^{\lambda b} - e^{\lambda a} \right) = \frac{e^{\lambda b} - e^{\lambda a}}{\lambda (b - a)}.
\]

This completes the proof that the moment generating function of a uniform random variable over \([a, b]\) is:
\[
M_X(\lambda) = \frac{e^{\lambda b} - e^{\lambda a}}{\lambda (b - a)}
\]




\end{enumerate}
















\newpage
\section*{Exercise 2}
\begin{enumerate}
\item \textcolor{blueColor}{
  Show that if \(X_i\) is \(\sigma_i\)-SubGaussian for \(i = 1, 2\) then \(X_1 + X_2\) is \((\sigma_1 + \sigma_2)\)-SubGaussian
\footnote{Use the Hölder inequality \((\mathbb{E}[XY] \leq (\mathbb{E}[X^p])^{1/p} (\mathbb{E}[Y^q])^{1/q} \text{ if } \frac{1}{p} + \frac{1}{q} = 1 \text{ and } p, q \geq 0)\) 
on \(\mathbb{E}[e^{\lambda (X - \mathbb{E}[X])} e^{\lambda (Y - \mathbb{E}[Y])}]\)}.} 

Let \(X_1\) and \(X_2\) be \(\sigma_1\)-SubGaussian and \(\sigma_2\)-SubGaussian random variables, respectively. 

This means that for any \(\lambda \in \mathbb{R}\),
\[
\mathbb{E}\left[e^{\lambda (X_1 - \mathbb{E}[X_1])}\right] \leq e^{\frac{\lambda^2 \sigma_1^2}{2}} \quad \text{and} 
\quad \mathbb{E}\left[e^{\lambda (X_2 - \mathbb{E}[X_2])}\right] \leq e^{\frac{\lambda^2 \sigma_2^2}{2}}
\]

We need to show that \(X_1 + X_2\) is \((\sigma_1 + \sigma_2)\)-SubGaussian, i.e.,
\[
\mathbb{E}\left[e^{\lambda (X_1 + X_2 - \mathbb{E}[X_1 + X_2])}\right] \leq e^{\frac{\lambda^2 (\sigma_1 + \sigma_2)^2}{2}}
\]

Consider the expectation:
\[
\mathbb{E}\left[e^{\lambda (X_1 + X_2 - \mathbb{E}[X_1] - \mathbb{E}[X_2])}\right] = \mathbb{E}\left[e^{\lambda (X_1 - \mathbb{E}[X_1])} e^{\lambda (X_2 - \mathbb{E}[X_2])}\right]
\]

Using Hölder's inequality with \(p = q = 2\) (since \(\frac{1}{p} + \frac{1}{q} = 1\) and \(p, q \geq 0\)), we get:
\[
\mathbb{E}\left[e^{\lambda (X_1 - \mathbb{E}[X_1])} e^{\lambda (X_2 - \mathbb{E}[X_2])}\right] \leq 
\left(\mathbb{E}\left[e^{2\lambda (X_1 - \mathbb{E}[X_1])}\right]\right)^{1/2} \left(\mathbb{E}\left[e^{2\lambda (X_2 - \mathbb{E}[X_2])}\right]\right)^{1/2}
\]

Since \(X_1\) is \(\sigma_1\)-SubGaussian and \(X_2\) is \(\sigma_2\)-SubGaussian, we have:
\[
\mathbb{E}\left[e^{2\lambda (X_1 - \mathbb{E}[X_1])}\right] \leq e^{2\lambda^2 \sigma_1^2} \quad \text{and} 
\quad \mathbb{E}\left[e^{2\lambda (X_2 - \mathbb{E}[X_2])}\right] \leq e^{2\lambda^2 \sigma_2^2}
\]

Therefore,
\[
\mathbb{E}\left[e^{\lambda (X_1 + X_2 - \mathbb{E}[X_1 + X_2])}\right] \leq \left(e^{2\lambda^2 \sigma_1^2}\right)^{1/2} \left(e^{2\lambda^2 \sigma_2^2}\right)^{1/2} = e^{\lambda^2 \sigma_1^2} e^{\lambda^2 \sigma_2^2} = e^{\lambda^2 (\sigma_1^2 + \sigma_2^2)}.
\]

To show that \(X_1 + X_2\) is \((\sigma_1 + \sigma_2)\)-SubGaussian, we use the triangle inequality for the variance:
\[
\sigma_1^2 + \sigma_2^2 \leq (\sigma_1 + \sigma_2)^2
\]

Thus,
\[
\mathbb{E}\left[e^{\lambda (X_1 + X_2 - \mathbb{E}[X_1 + X_2])}\right] \leq e^{\lambda^2 (\sigma_1 + \sigma_2)^2}.
\]

Hence, \(X_1 + X_2\) is \((\sigma_1 + \sigma_2)\)-SubGaussian.











\newpage
\textcolor{blueColor}{
\item For a sub-Gaussian random variable \(X\), define \(\|X\|_{vp}\) as the minimal \(\sigma\) for which \(X\) is \(\sigma\)-SubGaussian. 
Show that \(\|\cdot\|_{vp}\) is a norm on the space of centered sub-Gaussian random variables. This norm is called the Proxy Variance norm and \(\|X\|_{vp}\) 
is called the optimal proxy variance of \(X\).
}

\bigbreak

To show that \(\|\cdot\|_{vp}\) is a norm, we need to verify the following properties for all centered sub-Gaussian random variables \(X\) and \(Y\):
\begin{enumerate}
  \item \textbf{Positivity}: \(\|X\|_{vp} \geq 0\) and \(\|X\|_{vp} = 0\) if and only if \(X = 0\) almost surely.
  \item \textbf{Homogeneity}: \(\|aX\|_{vp} = |a| \|X\|_{vp}\).
  \item \textbf{Triangle Inequality}: \(\|X + Y\|_{vp} \leq \|X\|_{vp} + \|Y\|_{vp}\).
\end{enumerate}

\paragraph{Positivity} 

By definition, \(\|X\|_{vp}\) is the minimal \(\sigma\) such that \(X\) is \(\sigma\)-SubGaussian. 

Since the variance of \(X\) is non-negative, \(\sigma\) must also be non-negative. Therefore, \(\|X\|_{vp} \geq 0\). 


If \(X = 0\) almost surely, then \(X\) is deterministically zero, meaning it has no variability and does not deviate from its mean. 
Therefore, it is trivially \(\sigma\)-SubGaussian for any \(\sigma\), and hence \(\|X\|_{vp} = 0\). 

Conversely, if \(\|X\|_{vp} = 0\), then by definition, for all \(\lambda \in \mathbb{R}\),
\[
\mathbb{E}\left[e^{\lambda X}\right] \leq e^{\frac{\lambda^2 \cdot 0^2}{2}} = 1
\]
The moment generating function of \(X\), \(\mathbb{E}\left[e^{\lambda X}\right]\), being less than or equal to 1 for all \(\lambda\) implies that \(X\) must be zero almost surely. This is because the only random variable with this property is the constant zero. If \(X\) had any non-zero value with non-zero probability, the expectation \(\mathbb{E}\left[e^{\lambda X}\right]\) would exceed 1 for some \(\lambda\). Hence, \(\|X\|_{vp} = 0\) implies that \(X = 0\) almost surely.




\paragraph{Homogeneity}
Let \(a \in \mathbb{R}\) and \(X\) be a centered sub-Gaussian random variable. We need to show that \(\|aX\|_{vp} = |a| \|X\|_{vp}\).

\textbf{Step 1: \(\|aX\|_{vp} \leq |a| \|X\|_{vp}\)}

Assume \(\|X\|_{vp} = \sigma\). This means that for all \(\lambda \in \mathbb{R}\),
\[
\mathbb{E}\left[e^{\lambda X}\right] \leq e^{\frac{\lambda^2 \sigma^2}{2}}
\]

We need to show that \(aX\) is \(|a|\sigma\)-SubGaussian. Consider the moment generating function of \(aX\):
\[
\mathbb{E}\left[e^{\lambda (aX)}\right] = \mathbb{E}\left[e^{a\lambda X}\right]
\]

Using the sub-Gaussian property of \(X\) and the fact that \(a\) is a constant and $a\lambda$ spans that same range as $\lambda$, we have:
\[
\mathbb{E}\left[e^{a\lambda X}\right] \leq e^{\frac{(a\lambda)^2 \sigma^2}{2}} = e^{\frac{\lambda^2 a^2 \sigma^2}{2}} = e^{\frac{\lambda^2 (|a|\sigma)^2}{2}}
\]

This shows that \(aX\) is \(|a|\sigma\)-SubGaussian. Therefore, $\|aX\|_{vp} \leq |a| \|X\|_{vp}$. 

\textbf{Step 2: \(\|aX\|_{vp} \geq |a| \|X\|_{vp}\)}

If $a = 0$, then $aX = 0$ almost surely, and $\|aX\|_{vp} = 0 = |a| \|X\|_{vp}$.

Otherwise, Assume \(aX\) is \(\tau\)-SubGaussian for some \(\tau \geq 0\). This means that for all \(\lambda \in \mathbb{R}\),
\[
\mathbb{E}\left[e^{\lambda (aX)}\right] \leq e^{\frac{\lambda^2 \tau^2}{2}}.
\]

Consider \(\lambda' = \frac{\lambda}{a}\):
\[
\mathbb{E}\left[e^{\lambda X}\right] = \mathbb{E}\left[e^{\lambda' aX}\right] \leq e^{\frac{(\lambda')^2 \tau^2}{2}} = e^{\frac{\lambda^2 \tau^2}{2a^2}}
\]

By the definition of the sub-Gaussian property of \(X\), we must have:
\[
\frac{\tau^2}{a^2} \geq \sigma^2 \quad \Rightarrow \quad \tau \geq |a| \sigma.
\]

Therefore, $\|aX\|_{vp} \geq |a| \|X\|_{vp}$.

Combining both steps, we have shown that \(\|aX\|_{vp} = |a| \|X\|_{vp}\).



\paragraph{Triangle Inequality}
Let \(X\) and \(Y\) be centered sub-Gaussian random variables with \(\|X\|_{vp} = \sigma_X\) and \(\|Y\|_{vp} = \sigma_Y\). 

From Exercise 2.1, we know that if \(X\) is \(\sigma_X\)-SubGaussian and \(Y\) is \(\sigma_Y\)-SubGaussian, 
then \(X + Y\) is \((\sigma_X + \sigma_Y)\)-SubGaussian. Therefore, the proxy variance norm satisfies the triangle inequality:
\begin{align*}
\mathbb{E}\left[e^{\lambda (X + Y)}\right] &\leq e^{\frac{\lambda^2 (\sigma_X + \sigma_Y)^2}{2} }  \Rightarrow \\
\|X + Y\|_{vp} &:= \min\{\sigma \mid \forall \lambda \in \mathbb{R}, \quad \mathbb{E}\left[e^{\lambda (X + Y)}\right] \leq e^{\frac{\lambda^2 \sigma^2}{2}}\} \leq \sigma_X + \sigma_Y \Rightarrow \\
\|X + Y\|_{vp} &\leq \|X\|_{vp} + \|Y\|_{vp}
\end{align*}

\bigbreak


Since the Proxy Variance operator \(\|\cdot\|_{vp}\) satisfies positivity, homogeneity, and the triangle inequality, it is a norm on the space of centered sub-Gaussian random variables.











\end{enumerate}

\newpage
\section*{Exercise 3}
\begin{enumerate}
\item \textcolor{blueColor}{Let \(X\) be a \(\sigma\)-SubGaussian random variable. Show that
\footnote{Hint: You can use the fact that for twice differentiable \( f \) and \( g \), we have that if \( f(0) = g(0) \), \( f'(0) = g'(0) \) and \( f(x) \leq g(x) \) then \( f''(0) \leq g''(0) \)}
\(\sigma \geq \sqrt{\mathrm{var}(X)}\).}

\bigbreak


Let \(Y = X - \mathbb{E}[X]\). Note that \(Y\) is a centered random variable, i.e., \(\mathbb{E}[Y] = 0\), and since \(X\) is \(\sigma\)-SubGaussian, \(Y\) is also \(\sigma\)-SubGaussian. This is because the sub-Gaussian property is invariant under shifts by the mean. Hence,
\[
\mathbb{E}\left[e^{\lambda Y}\right] \leq e^{\frac{\lambda^2 \sigma^2}{2}} \quad \text{for all } \lambda \in \mathbb{R}.
\]

Define the function \(f(\lambda) = \mathbb{E}[e^{\lambda Y}]\) and \(g(\lambda) = e^{\frac{\lambda^2 \sigma^2}{2}}\). 

We need to show that:
\[
\sqrt{\mathrm{var}(X)} \leq \sigma.
\]

To do this, consider the Taylor expansions of \(f(\lambda)\) and \(g(\lambda)\) around \(\lambda = 0\).

The Taylor expansions of \(f(\lambda)\) and \(g(\lambda)\) are:
\begin{align*}
f(\lambda) &= f(0) + f'(0) \lambda + \frac{f''(0)}{2} \lambda^2 + O(\lambda^3) \\
g(\lambda) &= g(0) + g'(0) \lambda + \frac{g''(0)}{2} \lambda^2 + O(\lambda^3)
\end{align*}

Now, calculate the derivatives at \(\lambda = 0\), utilizing the result from question 1.1:

\begin{align*}
f(0) &= \mathbb{E}[e^{0}] = 1 \\
f'(0) &= \frac{d}{d\lambda} \mathbb{E}[e^{\lambda Y}] \bigg|_{\lambda=0} \stackrel{\text{1.1}}{=} \mathbb{E}[Y] = 0 \\
f''(0) &= \frac{d^2}{d\lambda^2} \mathbb{E}[e^{\lambda Y}] \bigg|_{\lambda=0} = \mathbb{E}[Y^2] = \mathrm{var}(Y)  \stackrel{\text{Shifting R.V by constant}}{=} \mathrm{var}(X) \\
g(0) &= e^{0} = 1 \\
g'(0) &= \frac{d}{d\lambda} e^{\frac{\lambda^2 \sigma^2}{2}} \bigg|_{\lambda=0} = \lambda \sigma^2 e^{\frac{\lambda^2 \sigma^2}{2}} \bigg|_{\lambda=0} = 0 \\
g''(0) &= \frac{d^2}{d\lambda^2} e^{\frac{\lambda^2 \sigma^2}{2}} \bigg|_{\lambda=0} = \frac{d}{d\lambda} \left( \lambda \sigma^2 e^{\frac{\lambda^2 \sigma^2}{2}} \right) \bigg|_{\lambda=0} = \sigma^2 e^{\frac{\lambda^2 \sigma^2}{2}} + \lambda \sigma^2 \left( \sigma^2 e^{\frac{\lambda^2 \sigma^2}{2}} \right) \bigg|_{\lambda=0} = \sigma^2
\end{align*}

From the given hint, since \(f(0) = g(0)\), \(f'(0) = g'(0)\), and \(f(\lambda) \leq g(\lambda)\) for all \(\lambda \in \mathbb{R}\), we have:
\[
f''(0) \leq g''(0).
\]

Therefore,
\[
\mathrm{var}(X) \leq \sigma^2.
\]

Taking the square root of both sides, we get:
\[
\sqrt{\mathrm{var}(X)} \leq \sigma.
\]

This completes the proof.



















\newpage
\item \textcolor{blueColor}{If \(\|X\|_{vp} = \sqrt{\mathrm{var}(X)}\), then \(X\) is called strictly sub-Gaussian. 
Show that if \(X\) is uniform on \(\{-1, 1\}\), then it is strictly sub-Gaussian. Conclude that the bound in Hoeffding's lemma is optimal.}

\bigbreak

First, let's show that if \(X\) is uniform on \(\{-1, 1\}\), then it is strictly sub-Gaussian.

Given \(X\) is uniform on \(\{-1, 1\}\), the probability mass function is:
\[
\mathbb{P}(X = -1) = \mathbb{P}(X = 1) = \frac{1}{2}.
\]

The mean and variance of \(X\) are:
\[
\mathbb{E}[X] = 0, \quad \mathrm{var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = 1.
\]

The moment generating function (MGF) of \(X\) is:
\[
M_X(\lambda) = \mathbb{E}[e^{\lambda X}] = \frac{1}{2} e^{\lambda} + \frac{1}{2} e^{-\lambda} = \cosh(\lambda).
\]

For \(X\) to be \(\sigma\)-SubGaussian, we need for all \(\lambda \in \mathbb{R}\):
\[
\cosh(\lambda) \leq e^{\frac{\lambda^2 \sigma^2}{2}}.
\]

For this inequality to hold for all \(\lambda\), we need to equate the exponents on both sides. Consider \(\lambda = 0\):
\[
\cosh(0) = e^{0} = 1.
\]

Next, consider the general case for \(\lambda \neq 0\). Use the Taylor series expansions to equate terms:

1. The Taylor series expansion for \(\cosh(\lambda)\) is:
\[
\cosh(\lambda) = 1 + \frac{\lambda^2}{2!} + \frac{\lambda^4}{4!} + \cdots.
\]

2. The Taylor series expansion for \(e^{\frac{\lambda^2 \sigma^2}{2}}\) is:
\[
e^{\frac{\lambda^2 \sigma^2}{2}} = 1 + \frac{\lambda^2 \sigma^2}{2!} + \frac{(\lambda^2 \sigma^2)^2}{4!} + \cdots.
\]

For the series to be equal for all \(\lambda\), each term in the expansion must match. Let's equate the coefficients of \(\lambda^2\):
\[
\frac{\lambda^2}{2} = \frac{\lambda^2 \sigma^2}{2}.
\]

Solving for \(\sigma\):
\[
\frac{1}{2} = \frac{\sigma^2}{2} \quad \Rightarrow \quad \sigma^2 = 1 \quad \Rightarrow \quad \sigma = 1.
\]

Therefore, the equality $(*)$:
\[
\frac{e^{\lambda} + e^{-\lambda}}{2} = e^{\frac{\lambda^2 \sigma^2}{2}}
\]
holds for all \(\lambda\) if and only if \(\sigma = 1\).

Thus, \(X\) is strictly sub-Gaussian with \(\sigma = 1\), meaning \(\|X\|_{vp} = \sqrt{\mathrm{var}(X)} = 1\). This shows that if \(X\) is uniform on \(\{-1, 1\}\), then it is strictly sub-Gaussian.


Let $a \leq X \leq b$ be a random variable. Hoeffding's lemma states that $X$ is \(\frac{(a-b)}{2}\)-SubGaussian, i.e., for all \(\lambda \in \mathbb{R}\),
\[
\mathbb{E}\left[e^{\lambda (X - \mathbb{E}[X])}\right] \leq e^{\frac{\lambda^2 (b-a)^2}{8}}.
\]

Lets substitute $X$ to both sides of the inequality:

The left side becomes:
\[
\mathbb{E}\left[e^{\lambda (X - \mathbb{E}[X])}\right] = \mathbb{E}\left[e^{\lambda X}\right] = cosh(\lambda)
\]
The right side becomes:
\[
e^{\frac{\lambda^2 (b-a)^2}{8}} = e^{\frac{\lambda^2 (1-(-1))^2 }{8}} = e^{\frac{\lambda^2 4}{8}} = e^{\frac{\lambda^2 (\text{Var}(X))}{2}}
\]
and we have seen in $(*)$ a case where the inequality holds with equality. Therefore, the bound in Hoeffding's lemma is optimal.









\newpage
\item \textcolor{blueColor}{Show that a linear combination of independent strictly sub-Gaussians is strictly sub-Gaussian.}





Let \(X_1, X_2, \ldots, X_n\) be independent strictly sub-Gaussian random variables, and let \(a_1, a_2, \ldots, a_n\) be real coefficients. 
We need to show that the linear combination \(Y = \sum_{i=1}^n a_i X_i\) is strictly sub-Gaussian.

Since \(X_i\) are strictly sub-Gaussian, we have \(\|X_i\|_{vp} = \sqrt{\mathrm{var}(X_i)}\) for all \(i\). By definition, this means that for each \(X_i\),
\[
\mathbb{E}[e^{\lambda X_i}] \leq e^{\frac{\lambda^2 \mathrm{var}(X_i)}{2}} \quad \text{for all } \lambda \in \mathbb{R}
\]

Because the \(X_i\) are independent, the moment generating function (MGF) of their linear combination \(Y\) is:
\begin{align*}
M_Y(\lambda) = \mathbb{E}[e^{\lambda Y}] = \mathbb{E}\left[e^{\lambda \sum_{i=1}^n a_i X_i}\right] = \mathbb{E}\left[e^{\sum_{i=1}^n \lambda a_i X_i}\right] = 
\mathbb{E}\left[ \prod_{i=1}^n e^ {\lambda a_i X_i}\right] \stackrel{independency}{=} \prod_{i=1}^n \mathbb{E}\left[e^{\lambda a_i X_i}\right].
\end{align*}

For each \(X_i\), since it is strictly sub-Gaussian, we have:
\[
\mathbb{E}\left[e^{\lambda a_i X_i}\right] \leq e^{\frac{\lambda^2 a_i^2 \mathrm{var}(X_i)}{2}}
\]

Therefore,
\[
M_Y(\lambda) \leq \prod_{i=1}^n e^{\frac{\lambda^2 a_i^2 \mathrm{var}(X_i)}{2}} = e^{\frac{\lambda^2}{2} \sum_{i=1}^n a_i^2 \mathrm{var}(X_i)}.
\]

From question 1.1, being $\sigma$-SubGaussian is equivalent to having an MGF that is bounded by the MGF of a centered Gaussian with variance $\sigma^2$. 
Therefore, \(Y\) is sub-Gaussian with variance parameter \(\sum_{i=1}^n a_i^2 \mathrm{var}(X_i)\)

Next, we need to show that \(Y\) is strictly sub-Gaussian. To do this, we calculate the variance of \(Y\):
\[
\mathrm{var}(Y) = \mathrm{var}\left(\sum_{i=1}^n a_i X_i\right)
\]

Since the \(X_i\) are independent, the variance of their linear combination is:
\[
\mathrm{var}(Y) = \sum_{i=1}^n a_i^2 \mathrm{var}(X_i).
\]

Since we already showed that:
\[
M_Y(\lambda) \leq e^{\frac{\lambda^2 \mathrm{var}(Y)}{2}},
\]

we have:
\[
\mathbb{E}[e^{\lambda Y}] \leq e^{\frac{\lambda^2 \mathrm{var}(Y)}{2}} \quad \text{for all } \lambda \in \mathbb{R}.
\]

Therefore, the variance proxy norm of \(Y\) is:
\[
\|Y\|_{vp} = \sqrt{\mathrm{var}(Y)}.
\]

Hence, \(Y\) is strictly sub-Gaussian.













\newpage
\item \textcolor{blueColor}{Show that for any \(M \geq 1\), there is a random variable \(X\) with \(\mathrm{var}(X) = 1\) and \(\|X\|_{vp} = M\).}








\end{enumerate}

\newpage
\section*{Exercise 4}
\textcolor{blueColor}{
Show that there is a universal constant \(C > 0\) for which the following holds. If \(X\) is a random variable such that for any \(t \geq 0\),
\[
\Pr(X - \mathbb{E}[X] \geq t) \leq e^{-\frac{t^2}{2\sigma^2}} \quad \text{and} \quad \Pr(X - \mathbb{E}[X] \leq -t) \leq e^{-\frac{t^2}{2\sigma^2}}
\]
then \(X\) is \((C\sigma)\)-SubGaussian\footnote{Hint: You may use the fact that for a non-negative random variable \(Y\), \(\mathbb{E}[Y] = \int_0^\infty \Pr(Y \geq x)dx\)}.
}









\end{document}